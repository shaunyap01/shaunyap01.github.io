<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Potential Solutions for Harmful Deepfakes | Shaun Yap</title>
<meta name="keywords" content="##All Articles, #In-depth Essay, AI Ethics, Platform Regulation">
<meta name="description" content="Explore the growing threat of deepfakes and the ethical, legal, and societal challenges they pose. This in-depth article outlines practical solutions including AI regulation, platform accountability, detection tools, and media literacy to combat malicious synthetic media and misinformation online.">
<meta name="author" content="Shaun Yap">
<link rel="canonical" href="https://shaunyap01.github.io/articles/potential-solutions-for-harmful-deepfakes/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.36d52b074909f5ed06bfe1e273446dfd250966271f086b116f65b4adefaca78a.css" integrity="sha256-NtUrB0kJ9e0Gv&#43;Hic0Rt/SUJZicfCGsRb2W0re&#43;sp4o=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://shaunyap01.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://shaunyap01.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://shaunyap01.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://shaunyap01.github.io/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://shaunyap01.github.io/articles/potential-solutions-for-harmful-deepfakes/">
<meta property="og:title" content="Potential Solutions for Harmful Deepfakes" />
<meta property="og:description" content="Explore the growing threat of deepfakes and the ethical, legal, and societal challenges they pose. This in-depth article outlines practical solutions including AI regulation, platform accountability, detection tools, and media literacy to combat malicious synthetic media and misinformation online." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://shaunyap01.github.io/articles/potential-solutions-for-harmful-deepfakes/" />
<meta property="og:image" content="https://shaunyap01.github.io/article2-cover" /><meta property="article:section" content="articles" />
<meta property="article:published_time" content="2024-05-01T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-05-01T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://shaunyap01.github.io/article2-cover" />
<meta name="twitter:title" content="Potential Solutions for Harmful Deepfakes"/>
<meta name="twitter:description" content="Explore the growing threat of deepfakes and the ethical, legal, and societal challenges they pose. This in-depth article outlines practical solutions including AI regulation, platform accountability, detection tools, and media literacy to combat malicious synthetic media and misinformation online."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Articles",
      "item": "https://shaunyap01.github.io/articles/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Potential Solutions for Harmful Deepfakes",
      "item": "https://shaunyap01.github.io/articles/potential-solutions-for-harmful-deepfakes/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Potential Solutions for Harmful Deepfakes",
  "name": "Potential Solutions for Harmful Deepfakes",
  "description": "Explore the growing threat of deepfakes and the ethical, legal, and societal challenges they pose. This in-depth article outlines practical solutions including AI regulation, platform accountability, detection tools, and media literacy to combat malicious synthetic media and misinformation online.",
  "keywords": [
    "##All Articles", "#In-depth Essay", "AI Ethics", "Platform Regulation"
  ],
  "articleBody": " Description This essay critically examines the complex issue of harmful deepfakes - synthetic media created using AI to deceive, manipulate, or cause harm. It explores the serious risks deepfakes pose to individuals, institutions, and democratic societies, from financial fraud and reputational damage to political disinformation and civil unrest. Drawing on real-world incidents and current research, the essay argues that the unchecked spread of deepfakes threatens both personal security and public trust, particularly in contexts where authenticity is paramount, such as politics and journalism.\nRather than offering a single solution, the essay proposes a multifaceted response: regulating access to deepfake technologies, mandating automatic self-certification, incentivising the development of detection tools, and fostering a ‘zero-trust’ culture through public education. It also highlights the importance of platform responsibility and the need for new legal frameworks - especially in official and political communication - to prevent the normalisation of deception. At its core, the essay urges swift, coordinated action to address the ethical, legal, and societal challenges posed by this powerful and rapidly evolving technology.\nIntroduction Today’s generative artificial intelligence (AI) technology, using real image and sound data, can create synthetic media - ‘deepfakes’ - which are intentionally used to deceive others in order to cause harm or to gain an advantage. This essay seeks to answer the question, ‘What are potential solutions for harmful deepfakes?’\nThis essay is not intended to offer comprehensive solutions to the issue of harmful deepfakes; rather it seeks to explore and evaluate several paths in which harmful deepfakes might be defanged and who might be responsible for their implementation, taking into account social and ethical concerns, existing laws and regulations, and governance issues.\nI submit that adverse consequences caused by deepfakes can be mitigated - or even avoided - if such media can be readily identifiable as ‘fake’ from the onset. In order to make deepfakes readily identifiable, we must first consider how deepfakes are created and evaluate regulating the technology and service providers as well as access to the technology and service providers. We then consider platforms where harmful deepfakes can be communicated and evaluate regulating these as well as their access. Finally, educating the public must be an intrinsic part of society’s battle against harmful deepfakes.\nDeepfakes - A Relatively New Form of Data “The term deepfake is a portmanteau of ‘deep learning’ and ‘fake’” (Kaspersky, 2024). It refers to a form of synthetic media - such as images, videos, and audio - created by artificial intelligence (AI) that depicts non-existent scenarios or fictional events. The term gained prominence in 2017 when a Reddit moderator started a subreddit named ‘deepfakes’, where they posted videos using face-swapping technology to insert celebrities’ likenesses into existing pornographic footage (Britannica, 2024). Deepfakes bring the institutional commodification of personal data into sharp conflict with the individual desire for privacy - one’s likeness can now be falsely associated with porn! Even the prime minister of Italy is not spared this denigration (Orbán, 2024).\nThe ways deepfakes can deceive range from completely synthesised images (e.g. an unreal person or scene, created from multiple parts of others, that looks real) to changing expressions and body language of a real person (thereby deceiving the audience regarding the emotive state of the real person).\nThe potency of deepfake technology to deceive lies in its ability to create fakes that look and sound absolutely real as the following story illustrates.\nHK Big Fraud. In February 2024, the South China Morning Post (SCMP) disclosed that a multinational corporation experienced a HKD 200 million (USD 25.6 million) loss due to a sophisticated scam involving deepfake technology (Kong, 2024). One notable instance featured a multi-person video conference in which Hong Kong branch employees believed they were interacting with familiar company members, including the chief financial officer. However, it was later revealed that all participants, except for the unsuspecting employee, were artificially generated representations of real individuals. The fraudsters used deepfake technology to manipulate publicly available video and audio content, creating highly realistic imitations of the meeting’s participants.\nSocial \u0026 Ethical Concerns Losses or damage. HK Big Fraud demonstrates the potency of deepfake technology and is an example of harmful deepfakes where there are victims who suffer damages or losses such as financial losses (through scam or blackmail); relationship or job losses (through character assassination or reputational damage); health damage (through anxiety or distress); loss of peace and security (when civil disorder or hate crime occur because of fake news); even loss of civil liberties should an innocent person be convicted of crime because of deepfake. The victims may be individuals, targeted groups of individuals, businesses, even entire nations and society at large.\nNot all deepfakes are harmful. When considering methods to mitigate the dangers of malicious deepfakes, it is crucial to recognise that deepfake technology, which is capable of generating realistic but fake videos, images, or audio, offers significant advantages in sectors such as cinema, entertainment, and the arts, where audiences expect and accept a high degree of fictional content. We must build sufficient safeguards for social and ethical concerns while avoiding unintended consequences such as inadvertently curbing the development and use of deepfake technologies that can be beneficial to society, or unintentionally making access to the technology more difficult to the underprivileged in education.\nIt is in the public arena that harmful deepfakes are most dangerous. It is in fields such as news media, social media and politics where, contextually, their contents are expected to be primarily truthful, that deepfake technology raises the most social and ethical concerns, and the corresponding governance issues. Bad actors can now utilise deepfake technology to effectively disseminate fake news, hate speech and spread false narratives that can have broad real-life consequences including social unrest and civil disorder. If deepfakes were used to deceive the electorate - the parties who are cheated and suffer losses would be the voters, the politicians who didn’t win because of untruths, and ultimately the country might have lost the services of more capable leaders or leaders who truly represent more of the people.\nRegulating Deepfake Technology - Who can own or hire a gun? Considering the potency of deepfakes and their underlying technology to deceive and wreak havoc on society and the individual, it seems entirely appropriate to ask: Should such potentially harmful technology be so readily accessible to anyone who wants to use it?\nAccording to Kisters (Kisters, 2023), “deepfake technology has become more advanced and accessible. There are now several apps and websites that allow users to create deepfakes with minimal effort or technical expertise. Deepfakes can be made with just a few photos or videos, which are then fed into an AI algorithm to create a realistic-looking video or audio recording.” This information from Kisters is very disconcerting because the situation is akin to making firearms readily available to anyone who wants them. In Chidi’s news report in The Guardian (Chidi 2024), a deepfake video of a US state senator supporting a bill (in reality, the senator opposes the bill) cost only USD 50. “With a USD 1,000 version, your own mother wouldn’t be able to tell the difference” (Chidi 2024).\nBoth firearms and deepfakes can inflict serious - if not lethal - harm on their victims. This raises the question: just like there are gun laws to regulate firearms, why shouldn’t something as potentially dangerous as deepfake technology be regulated as to who can own the technology, and who can use the technology? The paragraphs below explore potential legislations covering those who want to own the technology and those who want to use the technology.\nRequiring automatic self-certification for deepfakes. Regulating the technology behind deepfakes could be the first step towards making deepfakes readily identifiable. Developers of the deepfake technology could be required to ensure that the technology automatically embeds, within its output, some form of digital certificate that identifies the output as a deepfake. Such a digital certificate might involve using blockchain technology and include a log detailing when and where the technology is used every time the technology is used on a particular set of media data. With the public’s security and safety in mind, this regulation seems ethically defensible against the higher costs that might be incurred by the developer-owners.\nProfessional and educational versions. Developers and service providers of the deepfake technology could also be required to keep appropriate customer logs; the intent is to facilitate tracking down bad actors. Regulations could also restrict access to the professional version of the technology to businesses or institutions whose audience would normally expect fiction in what they see or hear, or who genuinely need a version where “your own mother wouldn’t be able to tell the difference” (Chidi 2024), e.g. those in the entertainment industry. An educational version - one where it is easier to identify that the output is a fake - can be made available to the general public.\nHow would such regulations have impacted HK Big Fraud? In this case, because the deception occurred during supposedly ’live’ video conferencing, there was no option for the victim to examine media data for digital certificates. Nonetheless, such regulations would have made it significantly harder for the scammers to make good quality fakes.\nGovernment must facilitate development of deepfake-detection technology Given the very serious harms deepfakes can inflict upon society and the individual, the common layperson cannot be left to fend for themself in this deception war waged by harmful deepfakes. I would submit that if a deepfake is in the public arena - i.e. anyone in public can easily stumble upon this deepfake and be misled by it - then the government has responsibility to ensure that appropriate safeguards exist. One safeguard would be making technologies that can identify deepfakes readily available to the public. Government funding towards their development should be considered. This might take the form of research scholarships, government grants to certain organisations, or tax breaks for company research and development. While ensuring healthy competition, the government can also encourage collaboration among institutions and companies to develop and maintain deepfake-detection technology.\nRegulating Platforms that Might Host Deepfakes Besides regulating the technology - and access to that technology - that produces deepfakes, we must also consider the platforms or websites in the public arena where harmful deepfakes can be communicated and evaluate regulating these. The challenge is to find the appropriate balance between the ‘freedom of expression’ and costs of implementation versus protection of the individual’s reputation and right to privacy. Arguably, if an image or video uses deepfake technology, exposing this does not meaningfully harm the right to freedom of expression. The challenge, then, is to determine the rightful party who should be responsible for checking and labelling if an image or video is deepfake or not before it can be posted onto the public arena. In this regard, a multifaceted approach would likely work best, including the aforementioned government’s role in the development of deepfake-detection AI.\nThe platform could be required to exercise best endeavours to identify deepfakes posted on it. This might include use of deepfake-detection AI to automatically check and label images and videos uploaded onto the platform, and contributing to the development of deepfake-detection technology. In this regard, had mainstream media checked the source or the image of the official family picture released by the Princess of Wales in 2024 (Coughlan, 2024), the subsequent public debacle might have been averted when it was discovered that the picture had been digitally altered before release.\nIn addition, the platform could be required to encourage responsible truthful behaviours by its patrons, including reposters, while upholding the principles of free speech. To illustrate, the platform could make declaration of deepfakes, if posted, a condition of access to the platform and then suspend - or even ban - individuals who habitually post deepfakes without declaring them. Because of the limited resources of the common layperson, this route should only be taken if free deepfake-detection technology is readily available to the public; otherwise, it unfairly penalises the underprivileged.\nThird-party certification is a potential variation whereby companies can be encouraged to develop and implement technologies to identify deepfakes via legally required certification of videos and images communicated in the public domain. Companies that are authorised to provide certification (‘certification authorities’) must be specially licensed and they must keep an appropriate register of all videos and images that they have certified. Unlike the automatic self-certification by deepfake software which essentially declares its output is fake, this third-party certification certifies if the media has any element of deepfake or not. Clearly, if a particular media already has an embedded declaration that it is a fake, then it should have no requirement for third-party certification. The public can be encouraged to submit videos - barring porn and the likes which already have laws regulating them - with more harmful content (e.g. socially or politically inflammatory videos with greater risk of creating social unrest) for certification by keeping the service reasonably affordable to the general public through appropriate government subsidy. The vision is for anyone to be able to upload an image or video - or to provide the link to an image or video - for the certification authority to ascertain if the image or video contains deepfake and, if so, to what extent. Certification can use a star system and range from 1-star (almost all deepfake) to 5-star (no deepfake detected). Such a certification can be AI-automated because there is no qualitative judgement required regarding the degree of harm that might be caused by the deepfake.\nThis concept of third-party certification can be easily extended into a national searchable online register of the more harmful deepfakes which will allow the public to easily check if a video or image they have stumbled upon in the public arena is deepfake or not.\nEducation is an intricate part of the overall solution Public awareness and media literacy are essential countermeasures against AI-driven social engineering and manipulation attacks. From an early education, individuals should be taught to distinguish between genuine and fabricated content, comprehend the distribution methods of deepfakes, and recognise the psychological and social engineering tactics employed by malicious actors. Media literacy programs must emphasise critical thinking and provide the tools necessary to verify the information people consume.\nA ‘zero-trust’ approach should be instilled. This involves not automatically trusting any information by default and instead verifying everything. Applied to online information consumption, it encourages a healthy level of scepticism and continuous verification. This mindset complements mindfulness practices, prompting individuals to pause before reacting to emotionally provocative content and to engage with digital information deliberately and thoughtfully. Cultivating a zero-trust culture equips users to handle deepfake and other AI-driven cyber threats that are challenging to combat with technology alone. As our digital lives expand and the metaverse becomes a reality, adopting a zero-trust mindset will be increasingly important. In these immersive environments, being able to discern between what is real and what is artificial will be essential.\nUndeclared deepfakes must be banned in politics and official business What happens when someone well known and in authority, e.g. a well-known politician, uses deepfake to speak in languages or dialects - which they normally couldn’t - without telling the audience that deepfake technology is used? This was exactly what happened in 2023 when Eric Adams, Mayor of New York City, used deepfake robocalls of himself to convey a message in multiple languages without declaring that deepfake was used (Cieslak, 2024). The unintended consequence here - the potential problem - is that when deepfake is used to communicate official messages without telling the audience that it is deepfake, it becomes normalised for the public to expect that official ‘real’ images or videos may not be real. This normalisation is especially harmful because it will lead to severe erosion of trust and dysfunctional democracies with broader implications for health and societies (Bollyky, 2024). For this reason, laws that ban undeclared deepfake use in politics or official business must be supported. The fiasco surrounding the altered family photo released by the Princess of Wales in 2024 likely highlights growing societal concern about undeclared fakes issued officially (Coughlan, 2024). As at year-end 2023, there are woefully inadequate laws governing the use of AI and deepfakes in politics in both the UK (Mortimer, 2024) and US (Edelman, 2024). More must be done. Quickly.\nConclusion This essay seeks to contribute to the development of comprehensive solutions to the issue of harmful deepfakes by suggesting, in a holistic manner, several potential solutions underpinned at one end by requiring the AI technology behind deepfakes to self-certify each deepfake synthesised and, at the other end, by making deepfake-detection technology available to the masses. In between, making access to the technology more difficult for bad actors, restricting communication of harmful deepfakes in the public arena through certification requirements, making it easier for the common layperson to verify that a video they have seen is not deepfake. Education is an intricate part of a holistic solution and this includes fostering a culture of zero-trust mindset. Furthermore, banning undeclared deepfake use in politics or official business helps set a good example by leaders and can counter the declining levels of trust in government and mainstream media.\nWhen considering legal measures to defang harmful deepfakes, the right to freedom of expression and the right to security and safety - both basic human rights (United Nations, 1948) - must be balanced equitably. In this regard, it seems judicious to have greater weighting given to the potential loss of peace and security by society versus the potential loss of freedom of expression by the individual.\nThe AI technology behind deepfakes is growing rapidly. Given the very serious harms deepfakes can inflict upon society and the individual, the government must move quickly to legislate against harmful deepfakes, promote or fund technological development to detect them, and institute changes in education and awareness programs. Our leaders should not require a personal misfortune to energise them to do the right thing - now (Orbán, 2024).\nBibliography Beaulieu, A. \u0026 Leonelli, S. (2022) Data and Society – A Critical Introduction\nBollyky, T (2024) The Global Erosion of Trust and Democracy and Its Implications for Health and Societies ; https://www.cfr.org/project/global-erosion-trust-and-democracy-and-its-implications-health-and-societies\nBritannica (2024) Deepfake; https://www.britannica.com/technology/deepfake\nChidi, G. (2024) Georgia lawmakers are using an AI deepfake video to try to ban political deepfakes [The Guardian]; https://www.theguardian.com/us-news/2024/mar/20/georgia-ai-ban-political-campaigns-deepfakes\nCieslak, M. (2024) How AI and deepfakes are changing politics [BBC News]; https://www.youtube.com/watch?v=wxEpPin8MWw\nCollard, A. (2024) 4 ways to future-proof against deepfakes in 2024 and beyond [World Economic Forum]; https://www.weforum.org/agenda/2024/02/4-ways-to-future-proof-against-deepfakes-in-2024-and-beyond/\nCoughlan, S. (2024) Is the pressure on Kate after photo chaos unfair? [BBC]; https://www.bbc.com/news/uk-68539637\nCurrie, M. (2022) What are deepfakes and how are they impacting society? [Edinburgh Impact, University of Edinburgh]; https://impact.ed.ac.uk/opinion/what-are-deepfakes-and-how-are-they-impacting-society/\nEdelman, A (2024) States turn their attention to regulating AI and deepfakes as 2024 kicks off [NBC News]; https://www.nbcnews.com/politics/states-turn-attention-regulating-ai-deepfakes-2024-rcna135122\nGamage, et al. (2022) Analyzing Conversations of Deepfakes on Reddit and Exploring Societal Implications ; https://dl.acm.org/doi/pdf/10.1145/3491102.3517446\nHays, E. (2023) Unmasking Deception: Examining Notable Deepfake Incidents and Their Impact ; https://medium.com/@toddkslater/unmasking-deception-examining-notable-deepfake-incidents-and-their-impact-b75ab62b5e1\nHays, E. (2023) Defending the Truth: How Technology is Combating Deepfake Misinformation; https://medium.com/@toddkslater/defending-the-truth-how-technology-is-combating-deepfake-misinformation-f100d0073c81\nHemrajani, A. (2023) China’s New Legislation on Deepfakes: Should the Rest of Asia Follow Suit? [The Diplomat] https://thediplomat.com/2023/03/chinas-new-legislation-on-deepfakes-should-the-rest-of-asia-follow-suit/\nIBM (2023) Shedding light on AI bias with real world examples [IBM Data and AI Team]; https://www.ibm.com/blog/shedding-light-on-ai-bias-with-real-world-examples/\nJaiman, A. (2020) Positive Use Cases of Synthetic Media (aka Deepfakes); https://towardsdatascience.com/positive-use-cases-of-deepfakes-49f510056387\nKaspersky, (2024). Deepfake; https://encyclopedia.kaspersky.com/glossary/deepfake/\nKisters, S. (2023) Deceptive Tech: The Ethics of Deepfakes; https://originstamp.com/blog/deceptive-tech-the-ethics-of-deepfakes/\nKlingler, N. (2024) Deepfakes in the Real World – Applications and Ethics; https://viso.ai/deep-learning/deepfakes-in-the-real-world-applications-and-ethics/\nKong, H. (2024) ‘Everyone looked real’: multinational firm’s Hong Kong office loses HK$200 million after scammers stage deepfake video meeting [South China Morning Post]; https://www.scmp.com/news/hong-kong/law-and-crime/article/3250851/everyone-looked-real-multinational-firms-hong-kong-office-loses-hk200-million-after-scammers-stage\nLawson, A. (2023) A Look at Global Deepfake Regulation Approaches [Responsible Artificial Intelligence Institute]; https://www.responsible.ai/a-look-at-global-deepfake-regulation-approaches/\nMea Digital Evidence Integrity (2024) Deepfakes and Their Impact on Society; https://www.mea-integrity.com/deepfakes-and-their-impact-on-society/\nMortimer, J. (2024) Fact Checkers Slam Government Inaction on Political Deepfakes Ahead of General Election, Saying Laws ‘Not Fit for Purpose’; https://bylinetimes.com/2024/04/18/fact-checkers-slam-government-inaction-on-political-deepfakes-ahead-of-general-election-saying-laws-not-fit-for-purpose/\nOrbán, T. (2024) Meloni vs. Deepfakes: Italian Government Submits AI Law to Parliament; https://europeanconservative.com/articles/news/meloni-vs-deepfakes-italian-government-submits-ai-law-to-parliament/#:~:text=via%20Wikimedia%20Commons-,The%20law%20would%20punish%20illegal%20AI-generated%20content%20distribution%20with,to%20five%20years%20in%20prison.\u0026text=Rome%20has%20finished%20drafting%20what,the%20EU%27s%20own%20AI%20Act.\nOrganization for Social Media Safety (2024) Deepfake Technology; https://www.socialmediasafety.org/advocacy/deepfake-technology/#:~:text=Deepfake%20technology%20first%20appeared%20in,to%20create%20realistic%20fake%20videos\nPuutio, A \u0026 Timis, D. (2020) Deepfake democracy: Here’s how modern elections could be decided by fake news [World Economic Forum]; https://www.weforum.org/agenda/2020/10/deepfake-democracy-could-modern-elections-fall-prey-to-fiction/\nQuirk, C. (2023) The High Stakes of Deepfakes: The Growing Necessity of Federal Legislation to Regulate This Rapidly Evolving Technology [Princeton Legal Journal]; https://legaljournal.princeton.edu/the-high-stakes-of-deepfakes-the-growing-necessity-of-federal-legislation-to-regulate-this-rapidly-evolving-technology/\nReddy, R. (2023) 24 Deepfake Statistics – Current Trends, Growth, and Popularity (December 2023); https://contentdetector.ai/articles/deepfake-statistics\nReddy, R. (2023) 15 Apps \u0026 Websites to Make Deepfake Videos Like a Pro; https://techpp.com/2023/12/18/make-deepfake-videos-apps-websites/\nUnited Nations (1948) Universal Declaration of Human Rights; https://www.ohchr.org/en/universal-declaration-of-human-rights/illustrated-universal-declaration-human-rights [accessed 22 Apr 2024]\nWeiner, D. \u0026 Norden, L. (2023) Regulating AI Deepfakes and Synthetic Media in the Political Arena; https://www.brennancenter.org/our-work/research-reports/regulating-ai-deepfakes-and-synthetic-media-political-arena\nWest, K. (2024) I was deepfaked by my best friend [BBC File on 4]; https://www.bbc.com/news/uk-68673390\n",
  "wordCount" : "3471",
  "inLanguage": "en",
  "image":"https://shaunyap01.github.io/article2-cover","datePublished": "2024-05-01T00:00:00Z",
  "dateModified": "2024-05-01T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Shaun Yap"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://shaunyap01.github.io/articles/potential-solutions-for-harmful-deepfakes/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Shaun Yap",
    "logo": {
      "@type": "ImageObject",
      "url": "https://shaunyap01.github.io/favicon.ico"
    }
  }
}
</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
            {left: "\\begin{align}", right: "\\end{align}", display: true},
            {left: "\\begin{align*}", right: "\\end{align*}", display: true},
            {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
            {left: "\\begin{gather}", right: "\\end{gather}", display: true},
            {left: "\\begin{CD}", right: "\\end{CD}", display: true},
          ],
          throwOnError : false
        });
    });
</script>
 


</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://shaunyap01.github.io/" accesskey="h">
                <img src="https://shaunyap01.github.io/favicon.ico" alt="logo" aria-label="logo"
                    height="18"
                    width="18">Shaun Yap</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://shaunyap01.github.io/articles/">
                    <span>Articles</span>
                </a>
            </li>
            <li>
                <a href="https://shaunyap01.github.io/projects/">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="https://shaunyap01.github.io/tags/">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://shaunyap01.github.io/archive/">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://shaunyap01.github.io/contact/">
                    <span>Contact</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Potential Solutions for Harmful Deepfakes
    </h1>
    <div class="post-meta"><span title='2024-05-01 00:00:00 +0000 UTC'>01 May 2024</span>&nbsp;&middot;&nbsp;Shaun Yap

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li>
          <ul>
            <li><a href="#description">Description</a></li>
          </ul>
        </li>
        <li><a href="#introduction">Introduction</a></li>
        <li><a href="#deepfakes---a-relatively-new-form-of-data">Deepfakes - A Relatively New Form of Data</a></li>
        <li><a href="#social--ethical-concerns">Social &amp; Ethical Concerns</a></li>
        <li><a href="#regulating-deepfake-technology---who-can-own-or-hire-a-gun">Regulating Deepfake Technology - Who can own or hire a gun?</a></li>
        <li><a href="#government-must-facilitate-development-of-deepfake-detection-technology">Government must facilitate development of deepfake-detection technology</a></li>
        <li><a href="#regulating-platforms-that-might-host-deepfakes">Regulating Platforms that Might Host Deepfakes</a></li>
        <li><a href="#education-is-an-intricate-part-of-the-overall-solution">Education is an intricate part of the overall solution</a></li>
        <li><a href="#undeclared-deepfakes-must-be-banned-in-politics-and-official-business">Undeclared deepfakes must be banned in politics and official business</a></li>
        <li><a href="#conclusion">Conclusion</a></li>
        <li><a href="#bibliography">Bibliography</a></li>
      </ul>
    </li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><hr>
<h3 id="description">Description<a hidden class="anchor" aria-hidden="true" href="#description">#</a></h3>
<p>This essay critically examines the complex issue of harmful deepfakes - synthetic media created using AI to deceive, manipulate, or cause harm. It explores the serious risks deepfakes pose to individuals, institutions, and democratic societies, from financial fraud and reputational damage to political disinformation and civil unrest. Drawing on real-world incidents and current research, the essay argues that the unchecked spread of deepfakes threatens both personal security and public trust, particularly in contexts where authenticity is paramount, such as politics and journalism.</p>
<p>Rather than offering a single solution, the essay proposes a multifaceted response: regulating access to deepfake technologies, mandating automatic self-certification, incentivising the development of detection tools, and fostering a &lsquo;zero-trust&rsquo; culture through public education. It also highlights the importance of platform responsibility and the need for new legal frameworks - especially in official and political communication - to prevent the normalisation of deception. At its core, the essay urges swift, coordinated action to address the ethical, legal, and societal challenges posed by this powerful and rapidly evolving technology.</p>
<hr>
<h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>Today&rsquo;s generative artificial intelligence (AI) technology, using real image and sound data, can create synthetic media - &lsquo;deepfakes&rsquo; - which are intentionally used to deceive others in order to cause harm or to gain an advantage. This essay seeks to answer the question, &lsquo;<strong>What are potential solutions for harmful deepfakes?</strong>&rsquo;</p>
<p>This essay is not intended to offer comprehensive solutions to the issue of harmful deepfakes; rather it seeks to explore and evaluate several paths in which harmful deepfakes might be defanged and who might be responsible for their implementation, taking into account social and ethical concerns, existing laws and regulations, and governance issues.</p>
<p>I submit that adverse consequences caused by deepfakes can be mitigated - or even avoided - if such media can be readily identifiable as &lsquo;fake&rsquo; from the onset. In order to make deepfakes readily identifiable, we must first consider how deepfakes are created and evaluate regulating the technology and service providers as well as access to the technology and service providers. We then consider platforms where harmful deepfakes can be communicated and evaluate regulating these as well as their access. Finally, educating the public must be an intrinsic part of society&rsquo;s battle against harmful deepfakes.</p>
<h2 id="deepfakes---a-relatively-new-form-of-data">Deepfakes - A Relatively New Form of Data<a hidden class="anchor" aria-hidden="true" href="#deepfakes---a-relatively-new-form-of-data">#</a></h2>
<p>&ldquo;The term deepfake is a portmanteau of &lsquo;deep learning&rsquo; and &lsquo;fake&rsquo;&rdquo; (<a href="#Kaspersky-2024">Kaspersky, 2024</a>). It refers to a form of synthetic media - such as images, videos, and audio - created by artificial intelligence (AI) that depicts non-existent scenarios or fictional events. The term gained prominence in 2017 when a Reddit moderator started a subreddit named &lsquo;deepfakes&rsquo;, where they posted videos using face-swapping technology to insert celebrities&rsquo; likenesses into existing pornographic footage (<a href="#Britannica-2024">Britannica, 2024</a>). Deepfakes bring the institutional commodification of personal data into sharp conflict with the individual desire for privacy - one&rsquo;s likeness can now be falsely associated with porn! Even the prime minister of Italy is not spared this denigration (<a href="#Orban-2024">Orbán, 2024</a>).</p>
<p>The ways deepfakes can deceive range from completely synthesised images (e.g. an unreal person or scene, created from multiple parts of others, that looks real) to changing expressions and body language of a real person (thereby deceiving the audience regarding the emotive state of the real person).</p>
<p>The potency of deepfake technology to deceive lies in its ability to create fakes that look and sound absolutely real as the following story illustrates.</p>
<p><strong>HK Big Fraud</strong>. In February 2024, the South China Morning Post (SCMP) disclosed that a multinational corporation experienced a HKD 200 million (USD 25.6 million) loss due to a sophisticated scam involving deepfake technology (<a href="Kong-2024">Kong, 2024</a>). One notable instance featured a multi-person video conference in which Hong Kong branch employees believed they were interacting with familiar company members, including the chief financial officer. However, it was later revealed that all participants, except for the unsuspecting employee, were artificially generated representations of real individuals. The fraudsters used deepfake technology to manipulate publicly available video and audio content, creating highly realistic imitations of the meeting&rsquo;s participants.</p>
<h2 id="social--ethical-concerns">Social &amp; Ethical Concerns<a hidden class="anchor" aria-hidden="true" href="#social--ethical-concerns">#</a></h2>
<p><strong>Losses or damage</strong>. HK Big Fraud demonstrates the potency of deepfake technology and is an example of harmful deepfakes where there are victims who suffer damages or losses such as <u>financial</u> losses (through scam or blackmail); <u>relationship</u> or <u>job</u> losses (through character assassination or reputational damage); <u>health damage</u> (through anxiety or distress); loss of <u>peace and security</u> (when civil disorder or hate crime occur because of fake news); even loss of <u>civil liberties</u> should an innocent person be convicted of crime because of deepfake. The victims may be individuals, targeted groups of individuals, businesses, even entire nations and society at large.</p>
<p><strong>Not all deepfakes are harmful</strong>. When considering methods to mitigate the dangers of malicious deepfakes, it is crucial to recognise that deepfake technology, which is capable of generating realistic but fake videos, images, or audio, offers significant advantages in sectors such as cinema, entertainment, and the arts, where audiences expect and accept a high degree of fictional content. We must build sufficient safeguards for social and ethical concerns while avoiding <u>unintended consequences</u> such as inadvertently curbing the development and use of deepfake technologies that can be beneficial to society, or unintentionally making access to the technology more difficult to the underprivileged in education.</p>
<p><strong>It is in the public arena that harmful deepfakes are most dangerous</strong>. It is in fields such as news media, social media and politics where, contextually, their contents are expected to be primarily truthful, that deepfake technology raises the most <u>social and ethical concerns</u>, and the corresponding <u>governance issues</u>. Bad actors can now utilise deepfake technology to effectively disseminate fake news, hate speech and spread false narratives that can have broad real-life consequences including social unrest and civil disorder. If deepfakes were used to deceive the electorate - the parties who are cheated and suffer losses would be the voters, the politicians who didn&rsquo;t win because of untruths, and ultimately the country might have lost the services of more capable leaders or leaders who truly represent more of the people.</p>
<h2 id="regulating-deepfake-technology---who-can-own-or-hire-a-gun">Regulating Deepfake Technology - Who can own or hire a gun?<a hidden class="anchor" aria-hidden="true" href="#regulating-deepfake-technology---who-can-own-or-hire-a-gun">#</a></h2>
<p>Considering the potency of deepfakes and their underlying technology to deceive and wreak havoc on society and the individual, it seems entirely appropriate to ask: <em>Should such potentially harmful technology be so readily accessible to anyone who wants to use it?</em></p>
<p>According to Kisters (<a href="#Kisters-2023">Kisters, 2023</a>), &ldquo;deepfake technology has become more advanced and accessible. There are now several apps and websites that allow users to create deepfakes with minimal effort or technical expertise. Deepfakes can be made with just a few photos or videos, which are then fed into an AI algorithm to create a realistic-looking video or audio recording.&rdquo; This information from Kisters is very disconcerting because the situation is akin to making firearms readily available to anyone who wants them. In Chidi&rsquo;s news report in The Guardian (<a href="#Chidi-2024">Chidi 2024</a>), a deepfake video of a US state senator supporting a bill (in reality, the senator opposes the bill) cost only USD 50. &ldquo;With a USD 1,000 version, your own mother wouldn&rsquo;t be able to tell the difference&rdquo; (<a href="#Chidi-2024">Chidi 2024</a>).</p>
<p>Both firearms and deepfakes can inflict serious - if not lethal - harm on their victims. This raises the question: <em>just like there are gun laws to regulate firearms, why shouldn’t something as potentially dangerous as deepfake technology be regulated as to who can own the technology, and who can use the technology?</em> The paragraphs below explore potential legislations covering those who want to own the technology and those who want to use the technology.</p>
<p><strong>Requiring automatic self-certification for deepfakes</strong>. Regulating the technology behind deepfakes could be the first step towards making deepfakes readily identifiable. Developers of the deepfake technology could be required to ensure that the technology automatically embeds, within its output, some form of <em>digital certificate</em> that identifies the output as a deepfake. Such a digital certificate might involve using blockchain technology and include a log detailing when and where the technology is used every time the technology is used on a particular set of media data. With the public&rsquo;s security and safety in mind, this regulation seems ethically defensible against the higher costs that might be incurred by the developer-owners.</p>
<p><strong>Professional and educational versions</strong>. Developers and service providers of the deepfake technology could also be required to keep appropriate <em>customer logs</em>; the intent is to facilitate tracking down bad actors. Regulations could also <em>restrict access</em> to the professional version of the technology to businesses or institutions whose audience would normally expect fiction in what they see or hear, or who genuinely need a version where &ldquo;your own mother wouldn&rsquo;t be able to tell the difference&rdquo; (<a href="#Chidi-2024">Chidi 2024</a>), e.g. those in the entertainment industry. An educational version - one where it is easier to identify that the output is a fake - can be made available to the general public.</p>
<p>How would such regulations have impacted HK Big Fraud? In this case, because the deception occurred during supposedly &rsquo;live&rsquo; video conferencing, there was no option for the victim to examine media data for digital certificates. Nonetheless, such regulations would have made it significantly harder for the scammers to make good quality fakes.</p>
<h2 id="government-must-facilitate-development-of-deepfake-detection-technology">Government must facilitate development of deepfake-detection technology<a hidden class="anchor" aria-hidden="true" href="#government-must-facilitate-development-of-deepfake-detection-technology">#</a></h2>
<p>Given the very serious harms deepfakes can inflict upon society and the individual, the common layperson cannot be left to fend for themself in this deception war waged by harmful deepfakes. I would submit that if a deepfake is in the public arena - i.e. anyone in public can easily stumble upon this deepfake and be misled by it - then the government has responsibility to ensure that appropriate safeguards exist. One safeguard would be <em>making technologies that can identify deepfakes readily available to the public</em>. Government funding towards their development should be considered. This might take the form of research scholarships, government grants to certain organisations, or tax breaks for company research and development. While ensuring healthy competition, the government can also encourage collaboration among institutions and companies to develop and maintain deepfake-detection technology.</p>
<h2 id="regulating-platforms-that-might-host-deepfakes">Regulating Platforms that Might Host Deepfakes<a hidden class="anchor" aria-hidden="true" href="#regulating-platforms-that-might-host-deepfakes">#</a></h2>
<p>Besides regulating the technology - and access to that technology - that produces deepfakes, we must also consider the platforms or websites in the public arena where harmful deepfakes can be communicated and evaluate regulating these. The challenge is to find the appropriate balance between the &lsquo;freedom of expression&rsquo; and costs of implementation versus protection of the individual&rsquo;s reputation and right to privacy. Arguably, if an image or video uses deepfake technology, exposing this does not meaningfully harm the right to freedom of expression. The challenge, then, is to determine the rightful party who should be responsible for checking and labelling if an image or video is deepfake or not before it can be posted onto the public arena. In this regard, a multifaceted approach would likely work best, including the aforementioned government&rsquo;s role in the development of deepfake-detection AI.</p>
<p>The platform could be required to exercise best endeavours to identify deepfakes posted on it. This might include use of deepfake-detection AI to automatically check and label images and videos uploaded onto the platform, and contributing to the development of deepfake-detection technology. In this regard, had mainstream media checked the source or the image of the official family picture released by the Princess of Wales in 2024 (<a href="#Coughlan-2024">Coughlan, 2024</a>), the subsequent public debacle might have been averted when it was discovered that the picture had been digitally altered before release.</p>
<p>In addition, the platform could be required to encourage responsible truthful behaviours by its patrons, including reposters, while upholding the principles of free speech. To illustrate, the platform could make declaration of deepfakes, if posted, a condition of access to the platform and then suspend - or even ban - individuals who habitually post deepfakes without declaring them. Because of the limited resources of the common layperson, this route should only be taken if free deepfake-detection technology is readily available to the public; otherwise, it unfairly penalises the underprivileged.</p>
<p>Third-party certification is a potential variation whereby companies can be encouraged to develop and implement technologies to identify deepfakes via legally required certification of videos and images communicated in the public domain. Companies that are authorised to provide certification (&lsquo;certification authorities&rsquo;) must be specially licensed and they must keep an appropriate register of all videos and images that they have certified. Unlike the automatic self-certification by deepfake software which essentially declares its output is fake, this third-party certification certifies if the media has any element of deepfake or not. Clearly, if a particular media already has an embedded declaration that it is a fake, then it should have no requirement for third-party certification. The public can be encouraged to submit videos - barring porn and the likes which already have laws regulating them - with more harmful content (e.g. socially or politically inflammatory videos with greater risk of creating social unrest) for certification by keeping the service reasonably affordable to the general public through appropriate government subsidy. The vision is for anyone to be able to upload an image or video - or to provide the link to an image or video - for the certification authority to ascertain if the image or video contains deepfake and, if so, to what extent. Certification can use a star system and range from 1-star (almost all deepfake) to 5-star (no deepfake detected). Such a certification can be AI-automated because there is no qualitative judgement required regarding the degree of harm that might be caused by the deepfake.</p>
<p>This concept of third-party certification can be easily extended into a national searchable online register of the more harmful deepfakes which will allow the public to easily check if a video or image they have stumbled upon in the public arena is deepfake or not.</p>
<h2 id="education-is-an-intricate-part-of-the-overall-solution">Education is an intricate part of the overall solution<a hidden class="anchor" aria-hidden="true" href="#education-is-an-intricate-part-of-the-overall-solution">#</a></h2>
<p><strong>Public awareness and media literacy</strong> are essential countermeasures against AI-driven social engineering and manipulation attacks. From an early education, individuals should be taught to distinguish between genuine and fabricated content, comprehend the distribution methods of deepfakes, and recognise the psychological and social engineering tactics employed by malicious actors. Media literacy programs must emphasise critical thinking and provide the tools necessary to verify the information people consume.</p>
<p><strong>A &lsquo;zero-trust&rsquo; approach should be instilled</strong>. This involves not automatically trusting any information by default and instead verifying everything. Applied to online information consumption, it encourages a healthy level of scepticism and continuous verification. This mindset complements mindfulness practices, prompting individuals to pause before reacting to emotionally provocative content and to engage with digital information deliberately and thoughtfully. Cultivating a zero-trust culture equips users to handle deepfake and other AI-driven cyber threats that are challenging to combat with technology alone. As our digital lives expand and the metaverse becomes a reality, adopting a zero-trust mindset will be increasingly important. In these immersive environments, being able to discern between what is real and what is artificial will be essential.</p>
<h2 id="undeclared-deepfakes-must-be-banned-in-politics-and-official-business">Undeclared deepfakes must be banned in politics and official business<a hidden class="anchor" aria-hidden="true" href="#undeclared-deepfakes-must-be-banned-in-politics-and-official-business">#</a></h2>
<p>What happens when someone well known and in authority, e.g. a well-known politician, uses deepfake to speak in languages or dialects - which they normally couldn&rsquo;t - without telling the audience that deepfake technology is used? This was exactly what happened in 2023 when Eric Adams, Mayor of New York City, used deepfake robocalls of himself to convey a message in multiple languages without declaring that deepfake was used (<a href="#Cieslak-2024">Cieslak, 2024</a>). The unintended consequence here - the potential problem - is that when deepfake is used to communicate official messages without telling the audience that it is deepfake, it becomes normalised for the public to expect that official &lsquo;real&rsquo; images or videos may not be real. This normalisation is especially harmful because it will lead to severe erosion of trust and dysfunctional democracies with broader implications for health and societies (<a href="#Bollyky-2024">Bollyky, 2024</a>). For this reason, laws that ban undeclared deepfake use in politics or official business must be supported. The fiasco surrounding the altered family photo released by the Princess of Wales in 2024 likely highlights growing societal concern about undeclared fakes issued officially (<a href="#Coughlan-2024">Coughlan, 2024</a>). As at year-end 2023, there are woefully inadequate laws governing the use of AI and deepfakes in politics in both the UK (<a href="#Mortimer-2024">Mortimer, 2024</a>) and US (<a href="#Edelman-2024">Edelman, 2024</a>). More must be done. Quickly.</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>This essay seeks to contribute to the development of comprehensive solutions to the issue of harmful deepfakes by suggesting, in a holistic manner, several potential solutions underpinned at one end by requiring the AI technology behind deepfakes to self-certify each deepfake synthesised and, at the other end, by making deepfake-detection technology available to the masses. In between, making access to the technology more difficult for bad actors, restricting communication of harmful deepfakes in the public arena through certification requirements, making it easier for the common layperson to verify that a video they have seen is not deepfake. Education is an intricate part of a holistic solution and this includes fostering a culture of zero-trust mindset. Furthermore, banning undeclared deepfake use in politics or official business helps set a good example by leaders and can counter the declining levels of trust in government and mainstream media.</p>
<p>When considering legal measures to defang harmful deepfakes, the right to freedom of expression and the right to security and safety - both basic human rights (<a href="#UN-1948">United Nations, 1948</a>) - must be balanced equitably. In this regard, it seems judicious to have greater weighting given to the potential loss of peace and security by society versus the potential loss of freedom of expression by the individual.</p>
<p>The AI technology behind deepfakes is growing rapidly. Given the very serious harms deepfakes can inflict upon society and the individual, the government must move quickly to legislate against harmful deepfakes, promote or fund technological development to detect them, and institute changes in education and awareness programs. Our leaders should not require a personal misfortune to energise them to do the right thing - now (<a href="#Orban-2024">Orbán, 2024</a>).</p>
<hr>
<h2 id="bibliography">Bibliography<a hidden class="anchor" aria-hidden="true" href="#bibliography">#</a></h2>
<p>Beaulieu, A. &amp; Leonelli, S. (2022) Data and Society – A Critical Introduction</p>
<p><a id="Bollyky-2024"></a> Bollyky, T (2024) The Global Erosion of Trust and Democracy and Its Implications for Health and Societies ; <a href="https://www.cfr.org/project/global-erosion-trust-and-democracy-and-its-implications-health-and-societies" target="_blank">https://www.cfr.org/project/global-erosion-trust-and-democracy-and-its-implications-health-and-societies</a></p>
<p><a id="Britannica-2024"></a> Britannica (2024) Deepfake; <a href="https://www.britannica.com/technology/deepfake" target="_blank">https://www.britannica.com/technology/deepfake</a></p>
<p><a id="Chidi-2024"></a> Chidi, G. (2024) Georgia lawmakers are using an AI deepfake video to try to ban political deepfakes [The Guardian]; <a href="https://www.theguardian.com/us-news/2024/mar/20/georgia-ai-ban-political-campaigns-deepfakes" target="_blank">https://www.theguardian.com/us-news/2024/mar/20/georgia-ai-ban-political-campaigns-deepfakes</a></p>
<p><a id="cieslak-2024"></a> Cieslak, M. (2024) How AI and deepfakes are changing politics [BBC News]; <a href="https://www.youtube.com/watch?v=wxEpPin8MWw" target="_blank">https://www.youtube.com/watch?v=wxEpPin8MWw</a></p>
<p>Collard, A. (2024) 4 ways to future-proof against deepfakes in 2024 and beyond [World Economic Forum]; <a href="https://www.weforum.org/agenda/2024/02/4-ways-to-future-proof-against-deepfakes-in-2024-and-beyond/" target="_blank">https://www.weforum.org/agenda/2024/02/4-ways-to-future-proof-against-deepfakes-in-2024-and-beyond/</a></p>
<p><a id="Coughlan-2024"></a> Coughlan, S. (2024) Is the pressure on Kate after photo chaos unfair? [BBC]; <a href="https://www.bbc.com/news/uk-68539637" target="_blank">https://www.bbc.com/news/uk-68539637</a></p>
<p>Currie, M. (2022) What are deepfakes and how are they impacting society? [Edinburgh Impact, University of Edinburgh]; <a href="https://impact.ed.ac.uk/opinion/what-are-deepfakes-and-how-are-they-impacting-society/" target="_blank">https://impact.ed.ac.uk/opinion/what-are-deepfakes-and-how-are-they-impacting-society/</a></p>
<p><a id="Edelman-2024"></a> Edelman, A (2024) States turn their attention to regulating AI and deepfakes as 2024 kicks off [NBC News]; <a href="https://www.nbcnews.com/politics/states-turn-attention-regulating-ai-deepfakes-2024-rcna135122" target="_blank">https://www.nbcnews.com/politics/states-turn-attention-regulating-ai-deepfakes-2024-rcna135122</a></p>
<p>Gamage, et al. (2022) Analyzing Conversations of Deepfakes on Reddit and Exploring Societal Implications ; <a href="https://dl.acm.org/doi/pdf/10.1145/3491102.3517446" target="_blank">https://dl.acm.org/doi/pdf/10.1145/3491102.3517446</a></p>
<p>Hays, E. (2023) Unmasking Deception: Examining Notable Deepfake Incidents and Their Impact ; <a href="https://medium.com/@toddkslater/unmasking-deception-examining-notable-deepfake-incidents-and-their-impact-b75ab62b5e1" target="_blank">https://medium.com/@toddkslater/unmasking-deception-examining-notable-deepfake-incidents-and-their-impact-b75ab62b5e1</a></p>
<p>Hays, E. (2023) Defending the Truth: How Technology is Combating Deepfake Misinformation; <a href="https://medium.com/@toddkslater/defending-the-truth-how-technology-is-combating-deepfake-misinformation-f100d0073c81" target="_blank">https://medium.com/@toddkslater/defending-the-truth-how-technology-is-combating-deepfake-misinformation-f100d0073c81</a></p>
<p>Hemrajani, A. (2023) China’s New Legislation on Deepfakes: Should the Rest of Asia Follow Suit? [The Diplomat] <a href="https://thediplomat.com/2023/03/chinas-new-legislation-on-deepfakes-should-the-rest-of-asia-follow-suit/" target="_blank">https://thediplomat.com/2023/03/chinas-new-legislation-on-deepfakes-should-the-rest-of-asia-follow-suit/</a></p>
<p>IBM (2023) Shedding light on AI bias with real world examples [IBM Data and AI Team]; <a href="https://www.ibm.com/blog/shedding-light-on-ai-bias-with-real-world-examples/" target="_blank">https://www.ibm.com/blog/shedding-light-on-ai-bias-with-real-world-examples/</a></p>
<p>Jaiman, A. (2020) Positive Use Cases of Synthetic Media (aka Deepfakes); <a href="https://towardsdatascience.com/positive-use-cases-of-deepfakes-49f510056387" target="_blank">https://towardsdatascience.com/positive-use-cases-of-deepfakes-49f510056387</a></p>
<p><a id="Kaspersky-2024"></a> Kaspersky, (2024). Deepfake; <a href="https://encyclopedia.kaspersky.com/glossary/deepfake/" target="_blank">https://encyclopedia.kaspersky.com/glossary/deepfake/</a></p>
<p><a id="Kisters-2023"></a> Kisters, S. (2023) Deceptive Tech: The Ethics of Deepfakes; <a href="https://originstamp.com/blog/deceptive-tech-the-ethics-of-deepfakes/" target="_blank">https://originstamp.com/blog/deceptive-tech-the-ethics-of-deepfakes/</a></p>
<p>Klingler, N. (2024) Deepfakes in the Real World – Applications and Ethics; <a href="https://viso.ai/deep-learning/deepfakes-in-the-real-world-applications-and-ethics/" target="_blank">https://viso.ai/deep-learning/deepfakes-in-the-real-world-applications-and-ethics/</a></p>
<p><a id="Kong-2024"></a> Kong, H. (2024) ‘Everyone looked real’: multinational firm’s Hong Kong office loses HK$200 million after scammers stage deepfake video meeting [South China Morning Post]; <a href="https://www.scmp.com/news/hong-kong/law-and-crime/article/3250851/everyone-looked-real-multinational-firms-hong-kong-office-loses-hk200-million-after-scammers-stage" target="_blank">https://www.scmp.com/news/hong-kong/law-and-crime/article/3250851/everyone-looked-real-multinational-firms-hong-kong-office-loses-hk200-million-after-scammers-stage</a></p>
<p>Lawson, A. (2023) A Look at Global Deepfake Regulation Approaches [Responsible Artificial Intelligence Institute]; <a href="https://www.responsible.ai/a-look-at-global-deepfake-regulation-approaches/" target="_blank">https://www.responsible.ai/a-look-at-global-deepfake-regulation-approaches/</a></p>
<p>Mea Digital Evidence Integrity (2024) Deepfakes and Their Impact on Society; <a href="https://www.mea-integrity.com/deepfakes-and-their-impact-on-society/" target="_blank">https://www.mea-integrity.com/deepfakes-and-their-impact-on-society/</a></p>
<p><a id="Mortimer-2024"></a> Mortimer, J. (2024) Fact Checkers Slam Government Inaction on Political Deepfakes Ahead of General Election, Saying Laws ‘Not Fit for Purpose’; <a href="https://bylinetimes.com/2024/04/18/fact-checkers-slam-government-inaction-on-political-deepfakes-ahead-of-general-election-saying-laws-not-fit-for-purpose/" target="_blank">https://bylinetimes.com/2024/04/18/fact-checkers-slam-government-inaction-on-political-deepfakes-ahead-of-general-election-saying-laws-not-fit-for-purpose/</a></p>
<p><a id="Orban-2024"></a> Orbán, T. (2024) Meloni vs. Deepfakes: Italian Government Submits AI Law to Parliament; <a href="https://europeanconservative.com/articles/news/meloni-vs-deepfakes-italian-government-submits-ai-law-to-parliament/#:~:text=via%20Wikimedia%20Commons-,The%20law%20would%20punish%20illegal%20AI-generated%20content%20distribution%20with,to%20five%20years%20in%20prison.&amp;text=Rome%20has%20finished%20drafting%20what,the%20EU%27s%20own%20AI%20Act" target="_blank">https://europeanconservative.com/articles/news/meloni-vs-deepfakes-italian-government-submits-ai-law-to-parliament/#:~:text=via%20Wikimedia%20Commons-,The%20law%20would%20punish%20illegal%20AI-generated%20content%20distribution%20with,to%20five%20years%20in%20prison.&text=Rome%20has%20finished%20drafting%20what,the%20EU%27s%20own%20AI%20Act</a>.</p>
<p>Organization for Social Media Safety (2024) Deepfake Technology; <a href="https://www.socialmediasafety.org/advocacy/deepfake-technology/#:~:text=Deepfake%20technology%20first%20appeared%20in,to%20create%20realistic%20fake%20videos" target="_blank">https://www.socialmediasafety.org/advocacy/deepfake-technology/#:~:text=Deepfake%20technology%20first%20appeared%20in,to%20create%20realistic%20fake%20videos</a></p>
<p>Puutio, A &amp; Timis, D. (2020) Deepfake democracy: Here&rsquo;s how modern elections could be decided by fake news [World Economic Forum]; <a href="https://www.weforum.org/agenda/2020/10/deepfake-democracy-could-modern-elections-fall-prey-to-fiction/" target="_blank">https://www.weforum.org/agenda/2020/10/deepfake-democracy-could-modern-elections-fall-prey-to-fiction/</a></p>
<p>Quirk, C. (2023) The High Stakes of Deepfakes: The Growing Necessity of Federal Legislation to Regulate This Rapidly Evolving Technology [Princeton Legal Journal]; <a href="https://legaljournal.princeton.edu/the-high-stakes-of-deepfakes-the-growing-necessity-of-federal-legislation-to-regulate-this-rapidly-evolving-technology/" target="_blank">https://legaljournal.princeton.edu/the-high-stakes-of-deepfakes-the-growing-necessity-of-federal-legislation-to-regulate-this-rapidly-evolving-technology/</a></p>
<p>Reddy, R. (2023) 24 Deepfake Statistics – Current Trends, Growth, and Popularity (December 2023); <a href="https://contentdetector.ai/articles/deepfake-statistics" target="_blank">https://contentdetector.ai/articles/deepfake-statistics</a></p>
<p>Reddy, R. (2023) 15 Apps &amp; Websites to Make Deepfake Videos Like a Pro; <a href="https://techpp.com/2023/12/18/make-deepfake-videos-apps-websites/" target="_blank">https://techpp.com/2023/12/18/make-deepfake-videos-apps-websites/</a></p>
<p><a id="UN-1938"></a> United Nations (1948) Universal Declaration of Human Rights; <a href="https://www.ohchr.org/en/universal-declaration-of-human-rights/illustrated-universal-declaration-human-rights" target="_blank">https://www.ohchr.org/en/universal-declaration-of-human-rights/illustrated-universal-declaration-human-rights</a> [accessed 22 Apr 2024]</p>
<p>Weiner, D. &amp; Norden, L. (2023) Regulating AI Deepfakes and Synthetic Media in the Political Arena; <a href="https://www.brennancenter.org/our-work/research-reports/regulating-ai-deepfakes-and-synthetic-media-political-arena" target="_blank">https://www.brennancenter.org/our-work/research-reports/regulating-ai-deepfakes-and-synthetic-media-political-arena</a></p>
<p>West, K. (2024) I was deepfaked by my best friend [BBC File on 4]; <a href="https://www.bbc.com/news/uk-68673390" target="_blank">https://www.bbc.com/news/uk-68673390</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://shaunyap01.github.io/tags/%23%23all-articles/">##All Articles</a></li>
      <li><a href="https://shaunyap01.github.io/tags/%23in-depth-essay/">#In-Depth Essay</a></li>
      <li><a href="https://shaunyap01.github.io/tags/ai-ethics/">AI Ethics</a></li>
      <li><a href="https://shaunyap01.github.io/tags/platform-regulation/">Platform Regulation</a></li>
    </ul>
  </footer>
</article>
    </main>
    

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
