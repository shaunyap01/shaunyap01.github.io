<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Evaluating Environment &amp; Climate Truthfulness in Social Media using Deep Learning &amp; Large Language Models (LLMs) | Shaun Yap</title>
<meta name="keywords" content="`Python, ^^All Projects, Natural Language Processing (NLP), Feature Engineering, Web scrape, Deep Learning, Large Language Models (LLM), Environmental Misinformation">
<meta name="description" content="Awarded Best Dissertation in Cohort, this MSc project explores the detection of climate and environmental misinformation on social media using a comparative framework of traditional natural language processing techniques, deep learning, and Large Language Models (LLMs). Leveraging a web-scraped dataset from PolitiFact, the study highlights the superiority of CNNs trained on ordinal truthfulness data, with accuracy boosted from 80.1% to 84.0% through GPT-4o-driven feature augmentation. While LLMs enhanced contextual understanding and sentiment analysis, their time complexity posed practical limitations. The project contributes novel insights into model performance trade-offs, evaluation metrics tailored to ordinal classification, and the practical integration of LLMs for misinformation mitigation in climate discourse.">
<meta name="author" content="Shaun Yap">
<link rel="canonical" href="https://shaunyap01.github.io/projects/evaluating-environment-and-climate-truthfulness-in-social-media-using-deep-learning-and-large-language-models/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.3fc93563987a6232a5b8b3e0a25a366c767e125a9f29a8d5ff271f363f5ca833.css" integrity="sha256-P8k1Y5h6YjKluLPgolo2bHZ&#43;ElqfKajV/ycfNj9cqDM=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://shaunyap01.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://shaunyap01.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://shaunyap01.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://shaunyap01.github.io/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://shaunyap01.github.io/projects/evaluating-environment-and-climate-truthfulness-in-social-media-using-deep-learning-and-large-language-models/">
<meta property="og:title" content="Evaluating Environment &amp; Climate Truthfulness in Social Media using Deep Learning &amp; Large Language Models (LLMs)" />
<meta property="og:description" content="Awarded Best Dissertation in Cohort, this MSc project explores the detection of climate and environmental misinformation on social media using a comparative framework of traditional natural language processing techniques, deep learning, and Large Language Models (LLMs). Leveraging a web-scraped dataset from PolitiFact, the study highlights the superiority of CNNs trained on ordinal truthfulness data, with accuracy boosted from 80.1% to 84.0% through GPT-4o-driven feature augmentation. While LLMs enhanced contextual understanding and sentiment analysis, their time complexity posed practical limitations. The project contributes novel insights into model performance trade-offs, evaluation metrics tailored to ordinal classification, and the practical integration of LLMs for misinformation mitigation in climate discourse." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://shaunyap01.github.io/projects/evaluating-environment-and-climate-truthfulness-in-social-media-using-deep-learning-and-large-language-models/" /><meta property="article:section" content="projects" />
<meta property="article:published_time" content="2024-09-01T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-09-01T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Evaluating Environment &amp; Climate Truthfulness in Social Media using Deep Learning &amp; Large Language Models (LLMs)"/>
<meta name="twitter:description" content="Awarded Best Dissertation in Cohort, this MSc project explores the detection of climate and environmental misinformation on social media using a comparative framework of traditional natural language processing techniques, deep learning, and Large Language Models (LLMs). Leveraging a web-scraped dataset from PolitiFact, the study highlights the superiority of CNNs trained on ordinal truthfulness data, with accuracy boosted from 80.1% to 84.0% through GPT-4o-driven feature augmentation. While LLMs enhanced contextual understanding and sentiment analysis, their time complexity posed practical limitations. The project contributes novel insights into model performance trade-offs, evaluation metrics tailored to ordinal classification, and the practical integration of LLMs for misinformation mitigation in climate discourse."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Projects",
      "item": "https://shaunyap01.github.io/projects/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Evaluating Environment \u0026 Climate Truthfulness in Social Media using Deep Learning \u0026 Large Language Models (LLMs)",
      "item": "https://shaunyap01.github.io/projects/evaluating-environment-and-climate-truthfulness-in-social-media-using-deep-learning-and-large-language-models/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Evaluating Environment \u0026 Climate Truthfulness in Social Media using Deep Learning \u0026 Large Language Models (LLMs)",
  "name": "Evaluating Environment \u0026 Climate Truthfulness in Social Media using Deep Learning \u0026 Large Language Models (LLMs)",
  "description": "Awarded Best Dissertation in Cohort, this MSc project explores the detection of climate and environmental misinformation on social media using a comparative framework of traditional natural language processing techniques, deep learning, and Large Language Models (LLMs). Leveraging a web-scraped dataset from PolitiFact, the study highlights the superiority of CNNs trained on ordinal truthfulness data, with accuracy boosted from 80.1% to 84.0% through GPT-4o-driven feature augmentation. While LLMs enhanced contextual understanding and sentiment analysis, their time complexity posed practical limitations. The project contributes novel insights into model performance trade-offs, evaluation metrics tailored to ordinal classification, and the practical integration of LLMs for misinformation mitigation in climate discourse.",
  "keywords": [
    "`Python", "^^All Projects", "Natural Language Processing (NLP)", "Feature Engineering", "Web scrape", "Deep Learning", "Large Language Models (LLM)", "Environmental Misinformation"
  ],
  "articleBody": "MSc Data Science \u0026 Statistics – University of Exeter\nRecipient of MSc Project Award for Best Dissertation in Cohort\nResearch Objectives The central aim of my Master’s dissertation is to advance the development of machine learning tools capable of identifying climate and environmental misinformation circulating on social media. In an age where accurate scientific communication is essential, this research investigates how effectively different modelling approaches can discern the truthfulness of climate-related claims.\nTo achieve this, the project was guided by three core objectives:\nEvaluate the effectiveness of Large Language Models (LLMs), such as OpenAI’s GPT-4o, in detecting climate and environmental misinformation. This involved assessing their classification performance and exploring their potential to augment conventional models through advanced sentiment and readability analysis.\nAssess the performance of other deep learning architectures, including Convolutional Neural Networks (CNNs), in classifying the truthfulness of online climate statements, particularly in comparison to LLMs and traditional NLP approaches.\nBenchmark traditional machine learning models (e.g. random forests, gradient boosting classifiers, support vector machines) to establish foundational performance metrics and understand their limitations in handling nuanced truthfulness labels.\nMethodology Data Collection and Preparation The dataset used in this research was web-scraped from a publicly available fact-checking resource PolitiFact, consisting of climate- and environment-related statements classified on an ordinal truthfulness scale: ‘Pants on Fire’, ‘False’, ‘Mostly False’, ‘Half True’, ‘Mostly True’, and ‘True’. This rich ordinal labelling allowed for a nuanced understanding of misinformation, rather than a simplistic binary approach. To facilitate comparative analysis, two distinct datasets were engineered:\nOrdinal Classification Dataset: Retains the ordinal clasification and combines ‘Pants on Fire’ into ‘False’. Binary Classification Dataset: Collapses the labels into two classes - ‘True’ and ‘False’ - following prior work in this domain. Exploratory Data Analysis (EDA) revealed subtle patterns in language use, punctuation, and sentiment distribution across truthfulness levels. These insights informed the subsequent feature engineering and modelling strategies.\nFeature Engineering Text-based features were derived using standard Natural Language Processing (NLP) tools, including:\nReadability: Calculated using Flesch-Kincaid reading ease scores. Sentiment Analysis: Utilised polarity and subjectivity scores generated via standard NLP library TextBlob. Custom Features: Initial trials included features such as punctuation frequency, based on observed correlations during EDA. However, this feature was excluded after statistical testing revealed limited generalisability beyond the dataset. To further enhance feature richness, the study introduced LLM-based augmentation using OpenAI’s GPT-4o. Each statement was assessed by GPT-4o for:\nReadability Polarity Subjectivity with scores normalised and averaged over three independent passes to reduce variability in outputs. This approach aimed to provide context-aware sentiment information beyond the capabilities of rule-based NLP tools. Evaluation Metrics Accurate evaluation of model performance is critical in the context of misinformation detection, where misclassifications - especially false positives - can have serious consequences. In this study, the evaluation strategy is carefully tailored to the type of task: ordinal classification, where labels have an inherent order, and binary classification, where labels are simplified into true/false categories. Different metrics were applied based on the classification type to ensure robust and meaningful assessments.\nMetrics for Ordinal Classification In ordinal classification, statements are categorised on a five-point truthfulness scale ranging from ‘False’ to ‘True’. Since the labels are ordered, standard accuracy metrics do not fully capture the quality of predictions. The following metrics were employed:\nOrdinal Classification Accuracy This metric measures the proportion of predictions that match the actual class exactly, providing a straightforward assessment of how often the model correctly classifies statements: $$\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$$\nRationale: This metric offers a direct and intuitive measure of the model’s overall performance by focusing on exact matches between predicted and actual classes.\nOrdinal Adjacent Accuracy Ordinal Classification Adjacent Accuracy extends the concept of accuracy by considering the proximity of the predicted class to the actual class. Specifically, we define this accuracy as the proportion of predictions that fall within one category of the true label: $$\\text{Adjacent Accuracy} = \\frac{\\text{Number of predictions within } \\pm 1 \\text{ class of actual}}{\\text{Total number of predictions}}$$\nRationale: This metric recognises that not all errors are equally severe in ordinal classification. For example, predicting “MOSTLY TRUE” when the correct label is “TRUE” is less problematic than predicting “FALSE”. By using Ordinal Classification Adjacent Accuracy, we capture the quality of predictions in a way that respects the ordered nature of the truthfulness ratings.\nMean Squared Error (MSE) Mean Squared Error is another metric used to evaluate ordinal classification by penalising predictions based on their distance from the actual label. $$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\nRationale: MSE is valuable during the training and validation phases as it penalises larger errors more heavily, helping to refine the model by minimising the distance between predicted and true classes. This focus on reducing error magnitude makes MSE an important metric for model optimisation. This metric will primarily be used for initial model fitting and validation.\nMetrics for Binary Classification Accuracy Accuracy is the most straightforward metric, representing the proportion of correctly classified instances among all instances. It provides a general sense of how well the model is performing. In the binary classification task, accuracy is calculated as: $$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\nRationale: While accuracy provides an overall view of model performance, it may not be sufficient in cases of imbalanced datasets or when the costs of different types of errors (i.e., false positives vs. false negatives) are not equal. However, it serves as a useful baseline metric to understand the general effectiveness of the model.\nPrecision Precision measures the proportion of true positive predictions among all positive predictions made by the model: $$\\text{Precision} = \\frac{TP}{TP + FP}$$\nRationale: Precision is particularly important in this context because it reflects the model’s ability to avoid false positives, which are especially costly when dealing with misinformation. In our scenario, a high precision ensures that when the model predicts a statement as true, it is likely to be accurate, thereby minimising the spread of misinformation.\nFalse Positive Rate (FPR) The False Positive Rate is the proportion of actual negatives that are incorrectly classified as positives: $$\\text{FPR} = \\frac{FP}{FP + TN}$$ Rationale: FPR is a critical metric in our application because it directly measures the rate at which false statements are incorrectly classified as true, which can lead to the propagation of misinformation. By closely monitoring the FPR, we can evaluate how well the model is mitigating the risk of false positives, which, in this context, is more damaging than false negatives. A primary objective of our modelling is to minimise the false positive rate.\nRecall Recall measures the proportion of actual true statements (positives) that are correctly identified by the model. It specifically focuses on the model’s ability to capture true statements, highlighting how well the model performs in avoiding false negatives (i.e., true statements incorrectly classified as false). $$\\text{Recall} = \\frac{TP}{TP + FN}$$ Rationale: While minimising the false positive rate is a primary objective, it is equally important to ensure that the model is not overly conservative, thereby failing to recognise true statements. Given the inherent trade-off between reducing type I errors (false positives) and type II errors (false negatives), tuning the model to decrease the FPR can result in a decrease in recall. A high recall ensures that the model effectively captures the true statements within the dataset, preventing valuable information from being overlooked. This is particularly vital in the context of climate change and environmental discourse, where underestimating the truthfulness of statements could lead to the dismissal of important facts and hinder public understanding.\nTo ensure a balanced approach, this study will require all models to achieve a minimum recall of 50%. This threshold guarantees that at least half of all true statements are correctly identified by the model, striking a necessary balance between minimising false positives and maintaining the model’s ability to accurately recognise true statements. By enforcing this minimum recall requirement, the model is held accountable not only for reducing misinformation but also for preserving the integrity of factual information, which is essential for building a trustworthy system for detecting climate-related truths and falsehoods.\nText Vectorisation Techniques The raw text data was transformed into numerical representations using a range of vectorisation techniques:\nTF-IDF (Term Frequency-Inverse Document Frequency) Bag of Words Word2Vec BERT Fine-tuned BERT Among these, TF-IDF was selected as the default method for most models due to its favourable trade-off between performance and computational cost. While models incorporating BERT showed promising results, the significantly higher computational overhead limited its use in large-scale model iterations.\nModelling Approaches The modelling pipeline involved training and evaluating three classes of models:\nTraditional Machine Learning Models: Logistic Regression (baseline) Random Forest Gradient Boosting Support Vector Machines (SVM) These models served as the baseline for performance comparison. Deep learning Models: Covolutional Neural Networks (CNNs) trained on embedded vector representations of the text. Fine-tuned transformer models where appropriate. CNNs were found to outperform traditional models, particularly on the ordinal classification dataset, which preserved more contextual truthfulness information.\nLarge Language Models (LLMs): GPT-4o was evaluated both as a data augmentation tool and as a standalone zero-shot classifier. While GPT-4o augmentation marginally improved performance for traditional models (Binary accuracy +1.9%), it had a significant impact on CNNs, improving binary classification accuracy from 80.1% to 85.0%.\nKey Results Feature engineering showed significant increase (~20%) in binary accuracy. Prior work in the domain collapsed the labels in to a binary classification and this work shows that there is a significant increase in performance (~10%) when training on the ordinal classification then converting back to binary labels. Tested LLMs as a data augmentation tool (theorised it could provide more nuanced, context-aware scores) and standalone zero-shot classifier. GPT-4o data augmentation showed a ~2% improvement on baseline model and a ~5% improvement on CNN model. Trade-off: GPT-4o enhanced data augmentation quality, but massively increased time complexity raises practical concerns for large-scale or real-time applications. ",
  "wordCount" : "1640",
  "inLanguage": "en",
  "datePublished": "2024-09-01T00:00:00Z",
  "dateModified": "2024-09-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Shaun Yap"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://shaunyap01.github.io/projects/evaluating-environment-and-climate-truthfulness-in-social-media-using-deep-learning-and-large-language-models/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Shaun Yap",
    "logo": {
      "@type": "ImageObject",
      "url": "https://shaunyap01.github.io/favicon.ico"
    }
  }
}
</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
            {left: "\\begin{align}", right: "\\end{align}", display: true},
            {left: "\\begin{align*}", right: "\\end{align*}", display: true},
            {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
            {left: "\\begin{gather}", right: "\\end{gather}", display: true},
            {left: "\\begin{CD}", right: "\\end{CD}", display: true},
          ],
          throwOnError : false
        });
    });
</script>
 


</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://shaunyap01.github.io/" accesskey="h">
                <img src="https://shaunyap01.github.io/favicon.ico" alt="logo" aria-label="logo"
                    height="18"
                    width="18">Shaun Yap</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://shaunyap01.github.io/research/">
                    <span>Research</span>
                </a>
            </li>
            <li>
                <a href="https://shaunyap01.github.io/articles/">
                    <span>Articles</span>
                </a>
            </li>
            <li>
                <a href="https://shaunyap01.github.io/projects/">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="https://shaunyap01.github.io/tags/">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://shaunyap01.github.io/archive/">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://shaunyap01.github.io/contact/">
                    <span>Contact</span>
                </a>
            </li>
            <li>
                <a href="https://shaunyap01.github.io/cv.pdf">
                    <span>Curriculum Vitae (CV)</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Evaluating Environment &amp; Climate Truthfulness in Social Media using Deep Learning &amp; Large Language Models (LLMs)
    </h1>
    <div class="post-meta"><span title='2024-09-01 00:00:00 +0000 UTC'>01 September 2024</span>&nbsp;&middot;&nbsp;Shaun Yap

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#research-objectives">Research Objectives</a></li>
    <li><a href="#methodology">Methodology</a>
      <ul>
        <li><a href="#data-collection-and-preparation">Data Collection and Preparation</a>
          <ul>
            <li><a href="#feature-engineering">Feature Engineering</a></li>
          </ul>
        </li>
        <li><a href="#evaluation-metrics">Evaluation Metrics</a>
          <ul>
            <li><a href="#metrics-for-ordinal-classification">Metrics for Ordinal Classification</a></li>
            <li><a href="#metrics-for-binary-classification">Metrics for Binary Classification</a></li>
          </ul>
        </li>
        <li><a href="#text-vectorisation-techniques">Text Vectorisation Techniques</a></li>
        <li><a href="#modelling-approaches">Modelling Approaches</a></li>
      </ul>
    </li>
    <li><a href="#key-results">Key Results</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><p>MSc Data Science &amp; Statistics – University of Exeter</p>
<p>Recipient of MSc Project Award for Best Dissertation in Cohort</p>
<hr>
<h1 id="research-objectives">Research Objectives<a hidden class="anchor" aria-hidden="true" href="#research-objectives">#</a></h1>
<p>The central aim of my Master’s dissertation is to advance the development of machine learning tools capable of identifying climate and environmental misinformation circulating on social media. In an age where accurate scientific communication is essential, this research investigates how effectively different modelling approaches can discern the truthfulness of climate-related claims.</p>
<p>To achieve this, the project was guided by three core objectives:</p>
<ol>
<li>
<p>Evaluate the effectiveness of Large Language Models (LLMs), such as OpenAI’s GPT-4o, in detecting climate and environmental misinformation. This involved assessing their classification performance and exploring their potential to augment conventional models through advanced sentiment and readability analysis.</p>
</li>
<li>
<p>Assess the performance of other deep learning architectures, including Convolutional Neural Networks (CNNs), in classifying the truthfulness of online climate statements, particularly in comparison to LLMs and traditional NLP approaches.</p>
</li>
<li>
<p>Benchmark traditional machine learning models (e.g. random forests, gradient boosting classifiers, support vector machines) to establish foundational performance metrics and understand their limitations in handling nuanced truthfulness labels.</p>
</li>
</ol>
<h1 id="methodology">Methodology<a hidden class="anchor" aria-hidden="true" href="#methodology">#</a></h1>
<h2 id="data-collection-and-preparation">Data Collection and Preparation<a hidden class="anchor" aria-hidden="true" href="#data-collection-and-preparation">#</a></h2>
<p>The dataset used in this research was web-scraped from a publicly available fact-checking resource <a href="https://www.politifact.com" target="_blank">PolitiFact</a>, consisting of climate- and environment-related statements classified on an ordinal truthfulness scale:
&lsquo;Pants on Fire&rsquo;, &lsquo;False&rsquo;, &lsquo;Mostly False&rsquo;, &lsquo;Half True&rsquo;, &lsquo;Mostly True&rsquo;, and &lsquo;True&rsquo;. This rich ordinal labelling allowed for a nuanced understanding of misinformation, rather than a simplistic binary approach.
To facilitate comparative analysis, two distinct datasets were engineered:</p>
<ul>
<li><strong>Ordinal Classification Dataset</strong>: Retains the ordinal clasification and combines &lsquo;Pants on Fire&rsquo; into &lsquo;False&rsquo;.</li>
<li><strong>Binary Classification Dataset</strong>: Collapses the labels into two classes - &lsquo;True&rsquo; and &lsquo;False&rsquo; - following prior work in this domain.</li>
</ul>
<p>Exploratory Data Analysis (EDA) revealed subtle patterns in language use, punctuation, and sentiment distribution across truthfulness levels. These insights informed the subsequent feature engineering and modelling strategies.</p>
<h3 id="feature-engineering">Feature Engineering<a hidden class="anchor" aria-hidden="true" href="#feature-engineering">#</a></h3>
<p>Text-based features were derived using standard Natural Language Processing (NLP) tools, including:</p>
<ul>
<li>Readability: Calculated using Flesch-Kincaid reading ease scores.</li>
<li>Sentiment Analysis: Utilised polarity and subjectivity scores generated via standard NLP library TextBlob.</li>
<li>Custom Features: Initial trials included features such as punctuation frequency, based on observed correlations during EDA. However, this feature was excluded after statistical testing revealed limited generalisability beyond the dataset.</li>
</ul>
<p>To further enhance feature richness, the study introduced LLM-based augmentation using OpenAI&rsquo;s GPT-4o. Each statement was assessed by GPT-4o for:</p>
<ul>
<li>Readability</li>
<li>Polarity</li>
<li>Subjectivity
with scores normalised and averaged over three independent passes to reduce variability in outputs. This approach aimed to provide context-aware sentiment information beyond the capabilities of rule-based NLP tools.</li>
</ul>
<h2 id="evaluation-metrics">Evaluation Metrics<a hidden class="anchor" aria-hidden="true" href="#evaluation-metrics">#</a></h2>
<p>Accurate evaluation of model performance is critical in the context of misinformation detection, where misclassifications - especially false positives - can have serious consequences. In this study, the evaluation strategy is carefully tailored to the type of task: <strong>ordinal classification</strong>, where labels have an inherent order, and <strong>binary classification</strong>, where labels are simplified into true/false categories. Different metrics were applied based on the classification type to ensure robust and meaningful assessments.</p>
<h3 id="metrics-for-ordinal-classification">Metrics for Ordinal Classification<a hidden class="anchor" aria-hidden="true" href="#metrics-for-ordinal-classification">#</a></h3>
<p>In ordinal classification, statements are categorised on a five-point truthfulness scale ranging from &lsquo;False&rsquo; to &lsquo;True&rsquo;. Since the labels are ordered, standard accuracy metrics do not fully capture the quality of predictions. The following metrics were employed:</p>
<ol>
<li>Ordinal Classification Accuracy
This metric measures the proportion of predictions that match the actual class exactly,
providing a straightforward assessment of how often the model correctly classifies
statements:</li>
</ol>
<p>$$\text{Accuracy} = \frac{\text{Number of correct predictions}}{\text{Total number of predictions}}$$</p>
<p>Rationale: This metric offers a direct and intuitive measure of the model&rsquo;s overall
performance by focusing on exact matches between predicted and actual classes.</p>
<ol start="2">
<li>Ordinal Adjacent Accuracy
Ordinal Classification Adjacent Accuracy extends the concept of accuracy by considering the
proximity of the predicted class to the actual class. Specifically, we define this accuracy as the proportion of predictions that fall within one category of the true label:</li>
</ol>
<p>$$\text{Adjacent Accuracy} = \frac{\text{Number of predictions within } \pm 1 \text{ class of actual}}{\text{Total number of predictions}}$$</p>
<p>Rationale: This metric recognises that not all errors are equally severe in ordinal
classification. For example, predicting &ldquo;MOSTLY TRUE&rdquo; when the correct label is &ldquo;TRUE&rdquo; is
less problematic than predicting &ldquo;FALSE&rdquo;. By using Ordinal Classification Adjacent Accuracy,
we capture the quality of predictions in a way that respects the ordered nature of the
truthfulness ratings.</p>
<ol start="3">
<li>Mean Squared Error (MSE)
Mean Squared Error is another metric used to evaluate ordinal classification by penalising
predictions based on their distance from the actual label.</li>
</ol>
<p>$$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$</p>
<p>Rationale: MSE is valuable during the training and validation phases as it penalises larger
errors more heavily, helping to refine the model by minimising the distance between
predicted and true classes. This focus on reducing error magnitude makes MSE an
important metric for model optimisation. This metric will primarily be used for initial model fitting and validation.</p>
<h3 id="metrics-for-binary-classification">Metrics for Binary Classification<a hidden class="anchor" aria-hidden="true" href="#metrics-for-binary-classification">#</a></h3>
<ol>
<li>Accuracy
Accuracy is the most straightforward metric, representing the proportion of correctly
classified instances among all instances. It provides a general sense of how well the model
is performing. In the binary classification task, accuracy is calculated as:</li>
</ol>
<p>$$\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$$</p>
<p>Rationale: While accuracy provides an overall view of model performance, it may not be
sufficient in cases of imbalanced datasets or when the costs of different types of errors (i.e., false positives vs. false negatives) are not equal. However, it serves as a useful baseline metric to understand the general effectiveness of the model.</p>
<ol start="2">
<li>Precision
Precision measures the proportion of true positive predictions among all positive predictions made by the model:</li>
</ol>
<p>$$\text{Precision} = \frac{TP}{TP + FP}$$</p>
<p>Rationale: Precision is particularly important in this context because it reflects the model&rsquo;s ability to avoid false positives, which are especially costly when dealing with misinformation. In our scenario, a high precision ensures that when the model predicts a statement as true, it is likely to be accurate, thereby minimising the spread of misinformation.</p>
<ol start="3">
<li>False Positive Rate (FPR)
The False Positive Rate is the proportion of actual negatives that are incorrectly classified as positives:
$$\text{FPR} = \frac{FP}{FP + TN}$$</li>
</ol>
<p>Rationale: FPR is a critical metric in our application because it directly measures the rate at which false statements are incorrectly classified as true, which can lead to the propagation of misinformation. By closely monitoring the FPR, we can evaluate how well the model is mitigating the risk of false positives, which, in this context, is more damaging than false negatives. A primary objective of our modelling is to minimise the false positive rate.</p>
<ol start="4">
<li>Recall
Recall measures the proportion of actual true statements (positives) that are correctly
identified by the model. It specifically focuses on the model’s ability to capture true
statements, highlighting how well the model performs in avoiding false negatives (i.e., true
statements incorrectly classified as false).
$$\text{Recall} = \frac{TP}{TP + FN}$$</li>
</ol>
<p>Rationale: While minimising the false positive rate is a primary objective, it is equally
important to ensure that the model is not overly conservative, thereby failing to recognise
true statements. Given the inherent trade-off between reducing type I errors (false positives) and type II errors (false negatives), tuning the model to decrease the FPR can result in a decrease in recall. A high recall ensures that the model effectively captures the true statements within the dataset, preventing valuable information from being overlooked. This is particularly vital in the context of climate change and environmental discourse, where underestimating the truthfulness of statements could lead to the dismissal of important facts and hinder public understanding.</p>
<p>To ensure a balanced approach, this study will require all models to achieve a minimum
recall of 50%. This threshold guarantees that at least half of all true statements are correctly identified by the model, striking a necessary balance between minimising false positives and maintaining the model&rsquo;s ability to accurately recognise true statements. By enforcing this minimum recall requirement, the model is held accountable not only for reducing misinformation but also for preserving the integrity of factual information, which is essential for building a trustworthy system for detecting climate-related truths and falsehoods.</p>
<h2 id="text-vectorisation-techniques">Text Vectorisation Techniques<a hidden class="anchor" aria-hidden="true" href="#text-vectorisation-techniques">#</a></h2>
<p>The raw text data was transformed into numerical representations using a range of vectorisation techniques:</p>
<ul>
<li>TF-IDF (Term Frequency-Inverse Document Frequency)</li>
<li>Bag of Words</li>
<li>Word2Vec</li>
<li>BERT</li>
<li>Fine-tuned BERT</li>
</ul>
<p>Among these, TF-IDF was selected as the default method for most models due to its favourable trade-off between performance and computational cost. While models incorporating BERT showed promising results, the significantly higher computational overhead limited its use in large-scale model iterations.</p>
<h2 id="modelling-approaches">Modelling Approaches<a hidden class="anchor" aria-hidden="true" href="#modelling-approaches">#</a></h2>
<p>The modelling pipeline involved training and evaluating three classes of models:</p>
<ol>
<li>Traditional Machine Learning Models:</li>
</ol>
<ul>
<li>Logistic Regression (baseline)</li>
<li>Random Forest</li>
<li>Gradient Boosting</li>
<li>Support Vector Machines (SVM)
These models served as the baseline for performance comparison.</li>
</ul>
<ol start="2">
<li>Deep learning Models:</li>
</ol>
<ul>
<li>Covolutional Neural Networks (CNNs) trained on embedded vector representations of the text.</li>
<li>Fine-tuned transformer models where appropriate.</li>
</ul>
<p>CNNs were found to outperform traditional models, particularly on the ordinal classification dataset, which preserved more contextual truthfulness information.</p>
<ol start="3">
<li>Large Language Models (LLMs):</li>
</ol>
<ul>
<li>GPT-4o was evaluated both as a data augmentation tool and as a standalone zero-shot classifier.</li>
</ul>
<p>While GPT-4o augmentation marginally improved performance for traditional models (Binary accuracy +1.9%), it had a significant impact on CNNs, improving binary classification accuracy from 80.1% to 85.0%.</p>
<h1 id="key-results">Key Results<a hidden class="anchor" aria-hidden="true" href="#key-results">#</a></h1>
<ul>
<li>Feature engineering showed significant increase (~20%) in binary accuracy.</li>
<li>Prior work in the domain collapsed the labels in to a binary classification and this work shows that there is a significant increase in performance (~10%) when training on the ordinal classification then converting back to binary labels.</li>
<li>Tested LLMs as a data augmentation tool (theorised it could provide more nuanced, context-aware scores) and standalone zero-shot classifier. GPT-4o data augmentation showed a ~2% improvement on baseline model and a ~5% improvement on CNN model.</li>
<li>Trade-off: GPT-4o enhanced data augmentation quality, but massively increased time complexity raises practical concerns for large-scale or real-time applications.</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://shaunyap01.github.io/tags/python/">`Python</a></li>
      <li><a href="https://shaunyap01.github.io/tags/all-projects/">^^All Projects</a></li>
      <li><a href="https://shaunyap01.github.io/tags/natural-language-processing-nlp/">Natural Language Processing (NLP)</a></li>
      <li><a href="https://shaunyap01.github.io/tags/feature-engineering/">Feature Engineering</a></li>
      <li><a href="https://shaunyap01.github.io/tags/web-scrape/">Web Scrape</a></li>
      <li><a href="https://shaunyap01.github.io/tags/deep-learning/">Deep Learning</a></li>
      <li><a href="https://shaunyap01.github.io/tags/large-language-models-llm/">Large Language Models (LLM)</a></li>
      <li><a href="https://shaunyap01.github.io/tags/environmental-misinformation/">Environmental Misinformation</a></li>
    </ul>
  </footer>
</article>
    </main>
    

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
