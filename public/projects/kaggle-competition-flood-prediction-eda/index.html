<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Kaggle Competition - Flood Prediction EDA | Shaun Yap</title>
<meta name="keywords" content="`Python, ^^All Projects, ^Kaggle Competition, Exploratory Data Analysis, Flood Prediction, Dimensionality Reduction, Data Visualisation, Large Dataset">
<meta name="description" content="Rigorous exploratory analysis of a large-scale (&gt;1,000,000 training datapoints) Kaggle flood prediction dataset. It highlights strong skills in handling high-dimensional structured data, performing scalable EDA with efficient visualisations, and applying both statistical and machine learning techniques for insight generation. Key competencies include dimensionality reduction (PCA, UMAP, t-SNE), correlation analysis, feature distribution comparison, and model interpretation using scikit-learn and statsmodels. The project also showcases custom cross-validation tooling, effective use of pipelines for reproducible modelling, and the derivation of a simplified additive model, reflecting a deep understanding of linear structures in high-volume data.">
<meta name="author" content="Shaun Yap">
<link rel="canonical" href="https://shaunyap01.github.io/projects/kaggle-competition-flood-prediction-eda/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.36d52b074909f5ed06bfe1e273446dfd250966271f086b116f65b4adefaca78a.css" integrity="sha256-NtUrB0kJ9e0Gv&#43;Hic0Rt/SUJZicfCGsRb2W0re&#43;sp4o=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://shaunyap01.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://shaunyap01.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://shaunyap01.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://shaunyap01.github.io/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://shaunyap01.github.io/projects/kaggle-competition-flood-prediction-eda/">
<meta property="og:title" content="Kaggle Competition - Flood Prediction EDA" />
<meta property="og:description" content="Rigorous exploratory analysis of a large-scale (&gt;1,000,000 training datapoints) Kaggle flood prediction dataset. It highlights strong skills in handling high-dimensional structured data, performing scalable EDA with efficient visualisations, and applying both statistical and machine learning techniques for insight generation. Key competencies include dimensionality reduction (PCA, UMAP, t-SNE), correlation analysis, feature distribution comparison, and model interpretation using scikit-learn and statsmodels. The project also showcases custom cross-validation tooling, effective use of pipelines for reproducible modelling, and the derivation of a simplified additive model, reflecting a deep understanding of linear structures in high-volume data." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://shaunyap01.github.io/projects/kaggle-competition-flood-prediction-eda/" /><meta property="article:section" content="projects" />
<meta property="article:published_time" content="2024-05-03T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-05-03T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Kaggle Competition - Flood Prediction EDA"/>
<meta name="twitter:description" content="Rigorous exploratory analysis of a large-scale (&gt;1,000,000 training datapoints) Kaggle flood prediction dataset. It highlights strong skills in handling high-dimensional structured data, performing scalable EDA with efficient visualisations, and applying both statistical and machine learning techniques for insight generation. Key competencies include dimensionality reduction (PCA, UMAP, t-SNE), correlation analysis, feature distribution comparison, and model interpretation using scikit-learn and statsmodels. The project also showcases custom cross-validation tooling, effective use of pipelines for reproducible modelling, and the derivation of a simplified additive model, reflecting a deep understanding of linear structures in high-volume data."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Projects",
      "item": "https://shaunyap01.github.io/projects/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Kaggle Competition - Flood Prediction EDA",
      "item": "https://shaunyap01.github.io/projects/kaggle-competition-flood-prediction-eda/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Kaggle Competition - Flood Prediction EDA",
  "name": "Kaggle Competition - Flood Prediction EDA",
  "description": "Rigorous exploratory analysis of a large-scale (\u0026gt;1,000,000 training datapoints) Kaggle flood prediction dataset. It highlights strong skills in handling high-dimensional structured data, performing scalable EDA with efficient visualisations, and applying both statistical and machine learning techniques for insight generation. Key competencies include dimensionality reduction (PCA, UMAP, t-SNE), correlation analysis, feature distribution comparison, and model interpretation using scikit-learn and statsmodels. The project also showcases custom cross-validation tooling, effective use of pipelines for reproducible modelling, and the derivation of a simplified additive model, reflecting a deep understanding of linear structures in high-volume data.",
  "keywords": [
    "`Python", "^^All Projects", "^Kaggle Competition", "Exploratory Data Analysis", "Flood Prediction", "Dimensionality Reduction", "Data Visualisation", "Large Dataset"
  ],
  "articleBody": " Link to the Kaggle Competition https://www.kaggle.com/competitions/playground-series-s4e5/overview\n# Standard library import datetime from collections import defaultdict # Data manipulation import numpy as np import pandas as pd # Visualisation import matplotlib.pyplot as plt from matplotlib.ticker import MaxNLocator import seaborn as sns # Machine learning and modelling from sklearn.base import clone from sklearn.linear_model import LinearRegression, Ridge from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor # Optional: if used later from sklearn.pipeline import make_pipeline, Pipeline from sklearn.model_selection import KFold from sklearn.preprocessing import StandardScaler, PolynomialFeatures, SplineTransformer from sklearn.metrics import r2_score from sklearn.decomposition import PCA from sklearn.manifold import TSNE # UMAP for dimensionality reduction import umap # Statistical modelling import statsmodels.api as sm Loading the Data We begin by loading the dataset and observing its structure. The training data consists of over one million rows and twenty features.\nKey Insights:\nGiven the dataset’s size, it’s important to select algorithms that scale efficiently with large volumes of data. Methods reliant on distance computations (e.g., k-nearest neighbors) or those involving kernel matrices may not be computationally feasible. To optimise training time, especially during experimentation, it may be practical to use a simple train–test split instead of full cross-validation. While five-fold cross-validation offers robust performance estimates, it can be prohibitively time-consuming at this scale. train = pd.read_csv('train.csv', index_col='id') test = pd.read_csv('test.csv', index_col='id') initial_features = list(test.columns) train MonsoonIntensity\rTopographyDrainage\rRiverManagement\rDeforestation\rUrbanization\rClimateChange\rDamsQuality\rSiltation\rAgriculturalPractices\rEncroachments\r...\rDrainageSystems\rCoastalVulnerability\rLandslides\rWatersheds\rDeterioratingInfrastructure\rPopulationScore\rWetlandLoss\rInadequatePlanning\rPoliticalFactors\rFloodProbability\rid\r0\r5\r8\r5\r8\r6\r4\r4\r3\r3\r4\r...\r5\r3\r3\r5\r4\r7\r5\r7\r3\r0.445\r1\r6\r7\r4\r4\r8\r8\r3\r5\r4\r6\r...\r7\r2\r0\r3\r5\r3\r3\r4\r3\r0.450\r2\r6\r5\r6\r7\r3\r7\r1\r5\r4\r5\r...\r7\r3\r7\r5\r6\r8\r2\r3\r3\r0.530\r3\r3\r4\r6\r5\r4\r8\r4\r7\r6\r8\r...\r2\r4\r7\r4\r4\r6\r5\r7\r5\r0.535\r4\r5\r3\r2\r6\r4\r4\r3\r3\r3\r3\r...\r2\r2\r6\r6\r4\r1\r2\r3\r5\r0.415\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r1117952\r3\r3\r4\r10\r4\r5\r5\r7\r10\r4\r...\r7\r8\r7\r2\r2\r1\r4\r6\r4\r0.495\r1117953\r2\r2\r4\r3\r9\r5\r8\r1\r3\r5\r...\r9\r4\r4\r3\r7\r4\r9\r4\r5\r0.480\r1117954\r7\r3\r9\r4\r6\r5\r9\r1\r3\r4\r...\r5\r5\r5\r5\r6\r5\r5\r2\r4\r0.485\r1117955\r7\r3\r3\r7\r5\r2\r3\r4\r6\r4\r...\r6\r8\r5\r3\r4\r6\r7\r6\r4\r0.495\r1117956\r4\r5\r6\r9\r5\r5\r2\r8\r4\r5\r...\r4\r8\r6\r5\r5\r6\r7\r7\r8\r0.560\r1117957 rows × 21 columns\nTarget Variable: FloodProbability Distribution The target variable, FloodProbability, ranges from 0.285 to 0.725. All values are discrete and occur in increments of 0.005, suggesting that the target has been finely quantised, likely as a result of post-processing or model calibration.\nTo visualise the distribution, we plot a histogram using a bin width of 0.005, ensuring that each unique target value is represented in its own bin. The resulting distribution is symmetric and closely resembles a normal distribution centered around 0.5, indicating a well-balanced target variable.\nbin_edges = np.arange(0.2825, 0.7300, 0.005) plt.figure(figsize=(8, 3)) plt.hist(train['FloodProbability'], bins=bin_edges, density=True, edgecolor='black', linewidth=0.05) plt.title('Distribution of FloodProbability') plt.xlabel('FloodProbability') plt.ylabel('Probability Mass') plt.grid(axis='y', linestyle='--', alpha=0.5) plt.tight_layout() plt.show() Feature Distributions All features in the dataset are integer-valued and exhibit right-skewed distributions, with the majority of values concentrated near the lower end of the range. Visual comparison of the training and test sets reveals that their feature distributions are virtually identical. Overlapping histograms show no meaningful divergence - if discrepancies existed, the bars representing each set (plotted in different colors) would be clearly distinguishable.\nimport matplotlib.pyplot as plt from matplotlib.ticker import MaxNLocator # Plot feature distributions for train vs test fig, axs = plt.subplots(5, 4, figsize=(14, 12)) # Slightly wider for better spacing for col, ax in zip(initial_features, axs.ravel()): # Relative frequency in training set train_dist = train[col].value_counts(normalize=True).sort_index() ax.bar(train_dist.index, train_dist.values, label='Train', alpha=1.0) # Relative frequency in test set test_dist = test[col].value_counts(normalize=True).sort_index() ax.bar(test_dist.index, test_dist.values, label='Test', alpha=0.6) ax.set_title(col) ax.xaxis.set_major_locator(MaxNLocator(integer=True)) ax.set_ylabel('Proportion') # Add legend only once, outside the grid handles, labels = ax.get_legend_handles_labels() fig.legend(handles, labels, loc='upper right', fontsize='medium') plt.tight_layout() plt.show() Correlation The correlation matrix reveals several interesting patterns:\nThere is virtually no linear correlation among the features - the pairwise correlation coefficients are all extremely close to zero. However, each feature exhibits a noticeable correlation with the target variable FloodProbability, suggesting that while features may be mutually uncorrelated, they individually contribute predictive information. A subtle and intriguing detail emerges from the correlation heatmap: none of the feature-feature correlations are exactly 0.0; instead, all are -0.0. While this might appear negligible, the consistent sign indicates that every pair of features is negatively correlated, albeit at an imperceptibly small scale. This observation hints at a form of weak statistical dependence across the feature set, despite the absence of meaningful linear correlation.\n# Prepare the list of features including the target corr_features = initial_features + ['FloodProbability'] # Compute correlation matrix correlation_matrix = np.corrcoef(train[corr_features], rowvar=False) # Plot heatmap plt.figure(figsize=(11, 11)) sns.heatmap( correlation_matrix, center=0, cmap='coolwarm', annot=True, fmt='.1f', xticklabels=corr_features, yticklabels=corr_features, square=True, linewidths=0.5, cbar_kws={'shrink': 0.8} ) plt.title('Correlation Matrix of Features and Target', fontsize=14) plt.tight_layout() plt.show() Dimensionality Reduction Principal Component Analysis (PCA) To assess the intrinsic dimensionality of the dataset, we apply Principal Component Analysis (PCA). The cumulative explained variance curve shows a gradual increase across components, indicating that the variance is distributed relatively evenly across all dimensions. This suggests that the dataset is effectively full-rank, and no small subset of principal components captures a dominant share of the variance.\nIn other words, linear dimensionality reduction would not be beneficial, as each feature contributes unique information that cannot be easily compressed without significant loss of signal.\n# Fit PCA on the feature set pca = PCA() pca.fit(train[initial_features]) # Plot cumulative explained variance plt.figure(figsize=(4, 3)) plt.plot(np.arange(1, len(initial_features) + 1), pca.explained_variance_ratio_.cumsum(), marker='o') plt.title('Principal Component Analysis', fontsize=12) plt.xlabel('Principal Component', fontsize=10) plt.ylabel('Cumulative Explained Variance', fontsize=10) plt.xticks(fontsize=9) plt.yticks([0, 0.5, 1.0], fontsize=9) plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True)) plt.grid(True, linestyle='--', alpha=0.5) plt.tight_layout() plt.show() UMAP We apply Unsupervised Uniform Manifold Approximation and Projection (UMAP) to reduce the high-dimensional feature space to two dimensions. UMAP is particularly useful for visualising structure, identifying clusters, or uncovering non-linear patterns that may not be evident in the raw feature space.\nHowever, in this case, the 2D projection does not reveal any clear structure or separation with respect to the target variable, FloodProbability. The data appears uniformly scattered, suggesting that UMAP does not expose any obvious low-dimensional manifold in this dataset.\n# Function to plot the embedding def plot_embedding(embedding, target, title): plt.figure(figsize=(6, 5)) plt.scatter( embedding[:, 0], embedding[:, 1], s=2, c=target, cmap='coolwarm', alpha=0.6 ) plt.gca().set_aspect('equal', 'datalim') plt.title(title, fontsize=14) plt.xlabel('UMAP Dimension 1') plt.ylabel('UMAP Dimension 2') plt.colorbar(label='FloodProbability') plt.tight_layout() plt.show() # Sample a subset for faster projection train_sample = train.sample(10_000) # Fit UMAP reducer = umap.UMAP() embedding = reducer.fit_transform(train_sample[initial_features]) # Plot the result plot_embedding( embedding, train_sample['FloodProbability'], 'Unsupervised UMAP Projection of the Training Dataset' ) Supervised UMAP Projection We also experiment with a supervised version of UMAP, which incorporates the target variable (FloodProbability) during the embedding process. This often enhances the separation of data based on the target and can reveal useful structure when relationships exist.\nIn this case, the resulting 2D projection does appear more organised visually. However, the structure largely reflects the quantised nature of the target, which consists of 83 discrete values. The apparent clustering is therefore an artifact of the target discretization rather than any meaningful data-driven separation. As a result, the supervised UMAP does not offer any additional insight beyond what was already known.\n# Sample a smaller subset for faster computation train_sample = train.sample(10000, random_state=42) # Fit Supervised UMAP using regression target reducer = umap.UMAP( n_neighbors=50, # smaller neighborhood for faster computation min_dist=0.5, # moderate compression target_metric='manhattan', target_weight=0.6, ) # Perform the supervised projection embedding = reducer.fit_transform( train_sample[initial_features], y=train_sample['FloodProbability'] ) # Plotting function def plot_embedding(embedding, target, title): plt.figure(figsize=(6, 5)) scatter = plt.scatter( embedding[:, 0], embedding[:, 1], s=2, c=target, cmap='coolwarm', alpha=0.7 ) plt.gca().set_aspect('equal', 'datalim') plt.title(title, fontsize=14) plt.xlabel('UMAP Dimension 1') plt.ylabel('UMAP Dimension 2') cbar = plt.colorbar(scatter) cbar.set_label('FloodProbability') plt.tight_layout() plt.show() # Visualise the result plot_embedding( embedding, train_sample['FloodProbability'], 'Supervised UMAP Projection of the Training Dataset' ) t-distributed Stochastic Neighbor Embedding (t-SNE) We also apply t-distributed Stochastic Neighbor Embedding (t-SNE) to project the high-dimensional feature space into two dimensions. t-SNE is a popular non-linear technique that is particularly effective at capturing local structure and revealing clusters.\nIn this case, however, the t-SNE projection does not uncover any meaningful patterns or structure in relation to the target variable, FloodProbability. The visualisation resembles random noise, indicating that t-SNE, like UMAP, fails to reveal a low-dimensional manifold in this dataset. Thus, t-SNE offers no additional insight for exploratory purposes.\n# Sample a subset for computational efficiency train_sample = train.sample(20_000, random_state=42) # Fit and transform with t-SNE reducer = TSNE(random_state=42, perplexity=30, max_iter=1000, learning_rate='auto') embedding = reducer.fit_transform(train_sample[initial_features]) # Plot the result plot_embedding( embedding, train_sample['FloodProbability'], '(Unsupervised) t-SNE Projection of the Training Dataset' ) Cross-Validation Strategy To ensure consistency and reproducibility across all model evaluations, we define a unified cross-validation function that standardises how models are trained and validated.\nFor efficiency during early experimentation, we evaluate performance using only the first fold of the five-fold cross-validation split. This approach significantly reduces computation time while providing a reasonable estimate of model performance. If higher precision is required later in the modeling pipeline, we can easily enable all five folds by adjusting a single flag.\nfrom collections import defaultdict # Configuration kf = KFold(n_splits=5, shuffle=True, random_state=1) SINGLE_FOLD = True # Toggle for full vs. single-fold CV COMPUTE_TEST_PRED = True # Toggle to compute test set predictions oof = defaultdict(lambda: np.full_like(train.FloodProbability, np.nan, dtype=float)) test_pred = {} def cross_validate(model, label, features=initial_features, n_repeats=1): \"\"\" Evaluate a model using cross-validation. Parameters: - model: scikit-learn compatible regressor - label: string identifier for the model - features: list of feature names to use - n_repeats: number of training repeats with different random seeds Outputs: - Stores out-of-fold predictions in `oof[label]` - Stores averaged test predictions in `test_pred[label]` if enabled \"\"\" start_time = datetime.datetime.now() scores = [] oof_preds = np.full_like(train.FloodProbability, np.nan, dtype=float) for fold, (idx_tr, idx_va) in enumerate(kf.split(train)): X_tr = train.iloc[idx_tr][features] X_va = train.iloc[idx_va][features] y_tr = train.iloc[idx_tr].FloodProbability y_va = train.iloc[idx_va].FloodProbability y_pred = np.zeros_like(y_va, dtype=float) for i in range(n_repeats): m = clone(model) if n_repeats \u003e 1: mm = m if isinstance(mm, Pipeline): mm = mm[-1] if isinstance(mm, TransformedTargetRegressor): mm = mm.regressor mm.set_params(random_state=i) m.fit(X_tr, y_tr) y_pred += m.predict(X_va) y_pred /= n_repeats score = r2_score(y_va, y_pred) print(f\"# Fold {fold}: R² = {score:.5f}\") scores.append(score) oof_preds[idx_va] = y_pred if SINGLE_FOLD: break elapsed_time = datetime.datetime.now() - start_time avg_score = np.mean(scores) print(f\"{Fore.GREEN}# Overall R²: {avg_score:.5f} | Model: {label}\" f\"{' (single fold)' if SINGLE_FOLD else ''} \" f\"| Time: {int(np.round(elapsed_time.total_seconds() / 60))} min{Style.RESET_ALL}\") oof[label] = oof_preds # Optional: generate test set predictions if COMPUTE_TEST_PRED: X_tr = train[features] y_tr = train.FloodProbability y_pred = np.zeros(len(test), dtype=float) for i in range(n_repeats): m = clone(model) if n_repeats \u003e 1: mm = m if isinstance(mm, Pipeline): mm = mm[-1] if isinstance(mm, TransformedTargetRegressor): mm = mm.regressor mm.set_params(random_state=i) m.fit(X_tr, y_tr) y_pred += m.predict(test[features]) y_pred /= n_repeats test_pred[label] = y_pred Linear Models We begin our modeling process with linear regression approaches, using various transformations to capture potential non-linearities in the data.\nA basic linear regression with standardised features provides a strong baseline. Adding polynomial features improves performance slightly, suggesting that some quadratic interactions among features are predictive. Applying spline transformations offers a more flexible form of non-linearity, but the performance gain is still modest. These results indicate that while the relationship between features and the target is not purely linear, simple transformations are not sufficient to fully capture the underlying patterns.\n# Standard Linear Regression model = make_pipeline( StandardScaler(), LinearRegression() ) cross_validate(model, 'LinearRegression') # Polynomial Features + Ridge Regression model = make_pipeline( StandardScaler(), PolynomialFeatures(degree=2, include_bias=False), Ridge() ) cross_validate(model, 'Poly-Ridge') # Spline Transformation + Ridge Regression model = make_pipeline( StandardScaler(), SplineTransformer(n_knots=5, degree=3), Ridge() ) cross_validate(model, 'Spline-Ridge') # Fold 0: R² = 0.84589\r\u001b[32m# Overall R²: 0.84589 | Model: LinearRegression (single fold) | Time: 0 min\u001b[0m\r# Fold 0: R² = 0.84642\r\u001b[32m# Overall R²: 0.84642 | Model: Poly-Ridge (single fold) | Time: 0 min\u001b[0m\r# Fold 0: R² = 0.84627\r\u001b[32m# Overall R²: 0.84627 | Model: Spline-Ridge (single fold) | Time: 0 min\u001b[0m\rLinear Regression with Statistical Inference As an alternative to scikit-learn’s LinearRegression, we use statsmodels, which provides detailed statistical diagnostics for linear models. This implementation allows us to inspect regression coefficients, along with their associated standard errors, t-statistics, and p-values.\nThis is particularly useful during exploratory analysis to:\nAssess the statistical significance of each feature, Interpret the magnitude and direction of feature effects, Check for potential multicollinearity or model instability. While the model’s predictive power is not the focus here, the regression summary offers valuable insights into how individual features relate to the target variable.\nimport statsmodels.api as sm # Add intercept term X = sm.add_constant(train[initial_features]) # Fit OLS model and display summary model = sm.OLS(train['FloodProbability'], X, missing='error') results = model.fit() results.summary() OLS Regression Results\rDep. Variable: FloodProbability R-squared: 0.845 Model: OLS Adj. R-squared: 0.845 Method: Least Squares F-statistic: 3.046e+05\rDate: Wed, 02 Apr 2025 Prob (F-statistic): 0.00 Time: 00:32:02 Log-Likelihood: 2.7820e+06\rNo. Observations: 1117957 AIC: -5.564e+06\rDf Residuals: 1117936 BIC: -5.564e+06\rDf Model: 20 Covariance Type: nonrobust coef std err t P\u003e|t| [0.025 0.975] const -0.0533 0.000 -234.995 0.000 -0.054 -0.053\rMonsoonIntensity 0.0056 9.25e-06 606.734 0.000 0.006 0.006\rTopographyDrainage 0.0056 9.09e-06 621.525 0.000 0.006 0.006\rRiverManagement 0.0057 9.18e-06 617.178 0.000 0.006 0.006\rDeforestation 0.0057 9.27e-06 612.404 0.000 0.006 0.006\rUrbanization 0.0057 9.14e-06 619.319 0.000 0.006 0.006\rClimateChange 0.0057 9.25e-06 612.437 0.000 0.006 0.006\rDamsQuality 0.0057 9.13e-06 619.170 0.000 0.006 0.006\rSiltation 0.0056 9.21e-06 612.284 0.000 0.006 0.006\rAgriculturalPractices 0.0056 9.2e-06 612.643 0.000 0.006 0.006\rEncroachments 0.0056 9.14e-06 618.374 0.000 0.006 0.006\rIneffectiveDisasterPreparedness 0.0056 9.16e-06 615.995 0.000 0.006 0.006\rDrainageSystems 0.0056 9.18e-06 613.641 0.000 0.006 0.006\rCoastalVulnerability 0.0057 9.11e-06 622.228 0.000 0.006 0.006\rLandslides 0.0056 9.15e-06 616.245 0.000 0.006 0.006\rWatersheds 0.0056 9.14e-06 617.853 0.000 0.006 0.006\rDeterioratingInfrastructure 0.0056 9.21e-06 609.647 0.000 0.006 0.006\rPopulationScore 0.0057 9.17e-06 618.914 0.000 0.006 0.006\rWetlandLoss 0.0056 9.2e-06 612.654 0.000 0.006 0.006\rInadequatePlanning 0.0056 9.14e-06 613.363 0.000 0.006 0.006\rPoliticalFactors 0.0056 9.1e-06 620.512 0.000 0.006 0.006\rOmnibus: 100155.250 Durbin-Watson: 2.000 Prob(Omnibus): 0.000 Jarque-Bera (JB): 148528.907\rSkew: 0.703 Prob(JB): 0.00 Kurtosis: 4.100 Cond. No. 265. Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\rAdditive Feature Structure An inspection of the linear regression coefficients reveals that all features have approximately the same multiplicative coefficient, estimated to be around 0.00565. This uniformity suggests that the model effectively reduces to a weighted sum of features with a common weight, plus an intercept term.\nThe model can be approximated as:\n$$ \\hat{y} = \\beta \\cdot \\sum_{i=1}^{d} x_i + \\alpha $$\nwhere:\n$\\beta \\approx 0.00565$, $\\alpha \\approx -0.053$, $x_i$ represents each feature, $d$ is the number of features. This implies that the sum of all feature values is a strong linear predictor of the target. Using this simplified form, we can compute an approximate prediction for FloodProbability:\nInsight: This surprisingly effective approximation highlights a latent structure in the dataset: the total feature magnitude strongly correlates with flood probability, even without modeling feature-specific effects.\n# Compute approximation using summed features approx_prediction = train[initial_features].sum(axis=1) * 0.00565 - 0.053 # Evaluate goodness of fit score = r2_score(train['FloodProbability'], approx_prediction) print(f\"R² Score (approximate model): {score:.5f}\") R² Score (approximate model): 0.84476\r",
  "wordCount" : "2591",
  "inLanguage": "en",
  "datePublished": "2024-05-03T00:00:00Z",
  "dateModified": "2024-05-03T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Shaun Yap"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://shaunyap01.github.io/projects/kaggle-competition-flood-prediction-eda/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Shaun Yap",
    "logo": {
      "@type": "ImageObject",
      "url": "https://shaunyap01.github.io/favicon.ico"
    }
  }
}
</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
            {left: "\\begin{align}", right: "\\end{align}", display: true},
            {left: "\\begin{align*}", right: "\\end{align*}", display: true},
            {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
            {left: "\\begin{gather}", right: "\\end{gather}", display: true},
            {left: "\\begin{CD}", right: "\\end{CD}", display: true},
          ],
          throwOnError : false
        });
    });
</script>
 


</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://shaunyap01.github.io/" accesskey="h">
                <img src="https://shaunyap01.github.io/favicon.ico" alt="logo" aria-label="logo"
                    height="18"
                    width="18">Shaun Yap</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://shaunyap01.github.io/articles/">
                    <span>Articles</span>
                </a>
            </li>
            <li>
                <a href="https://shaunyap01.github.io/projects/">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="https://shaunyap01.github.io/tags/">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://shaunyap01.github.io/archive/">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://shaunyap01.github.io/contact/">
                    <span>Contact</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Kaggle Competition - Flood Prediction EDA
    </h1>
    <div class="post-meta"><span title='2024-05-03 00:00:00 +0000 UTC'>03 May 2024</span>&nbsp;&middot;&nbsp;Shaun Yap

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li>
          <ul>
            <li><a href="#link-to-the-kaggle-competition">Link to the Kaggle Competition</a></li>
          </ul>
        </li>
        <li><a href="#loading-the-data">Loading the Data</a></li>
        <li><a href="#target-variable-floodprobability-distribution">Target Variable: <code>FloodProbability</code> Distribution</a></li>
        <li><a href="#feature-distributions">Feature Distributions</a></li>
        <li><a href="#correlation">Correlation</a></li>
        <li><a href="#dimensionality-reduction">Dimensionality Reduction</a>
          <ul>
            <li><a href="#principal-component-analysis-pca">Principal Component Analysis (PCA)</a></li>
            <li><a href="#umap">UMAP</a></li>
            <li><a href="#supervised-umap-projection">Supervised UMAP Projection</a></li>
            <li><a href="#t-distributed-stochastic-neighbor-embedding-t-sne">t-distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
          </ul>
        </li>
        <li><a href="#cross-validation-strategy">Cross-Validation Strategy</a></li>
        <li><a href="#linear-models">Linear Models</a>
          <ul>
            <li><a href="#linear-regression-with-statistical-inference">Linear Regression with Statistical Inference</a></li>
            <li><a href="#additive-feature-structure">Additive Feature Structure</a></li>
            <li><a href="#insight">Insight:</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><hr>
<h3 id="link-to-the-kaggle-competition">Link to the Kaggle Competition<a hidden class="anchor" aria-hidden="true" href="#link-to-the-kaggle-competition">#</a></h3>
<blockquote>
<p><a href="https://www.kaggle.com/competitions/playground-series-s4e5/overview" target="_blank">https://www.kaggle.com/competitions/playground-series-s4e5/overview</a></p></blockquote>
<hr>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Standard library</span>
</span></span><span style="display:flex;"><span><span style="color:#00a">import</span> <span style="color:#0aa;text-decoration:underline">datetime</span>
</span></span><span style="display:flex;"><span><span style="color:#00a">from</span> <span style="color:#0aa;text-decoration:underline">collections</span> <span style="color:#00a">import</span> defaultdict
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Data manipulation</span>
</span></span><span style="display:flex;"><span><span style="color:#00a">import</span> <span style="color:#0aa;text-decoration:underline">numpy</span> <span style="color:#00a">as</span> <span style="color:#0aa;text-decoration:underline">np</span>
</span></span><span style="display:flex;"><span><span style="color:#00a">import</span> <span style="color:#0aa;text-decoration:underline">pandas</span> <span style="color:#00a">as</span> <span style="color:#0aa;text-decoration:underline">pd</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Visualisation</span>
</span></span><span style="display:flex;"><span><span style="color:#00a">import</span> <span style="color:#0aa;text-decoration:underline">matplotlib.pyplot</span> <span style="color:#00a">as</span> <span style="color:#0aa;text-decoration:underline">plt</span>
</span></span><span style="display:flex;"><span><span style="color:#00a">from</span> <span style="color:#0aa;text-decoration:underline">matplotlib.ticker</span> <span style="color:#00a">import</span> MaxNLocator
</span></span><span style="display:flex;"><span><span style="color:#00a">import</span> <span style="color:#0aa;text-decoration:underline">seaborn</span> <span style="color:#00a">as</span> <span style="color:#0aa;text-decoration:underline">sns</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Machine learning and modelling</span>
</span></span><span style="display:flex;"><span><span style="color:#00a">from</span> <span style="color:#0aa;text-decoration:underline">sklearn.base</span> <span style="color:#00a">import</span> clone
</span></span><span style="display:flex;"><span><span style="color:#00a">from</span> <span style="color:#0aa;text-decoration:underline">sklearn.linear_model</span> <span style="color:#00a">import</span> LinearRegression, Ridge
</span></span><span style="display:flex;"><span><span style="color:#00a">from</span> <span style="color:#0aa;text-decoration:underline">sklearn.ensemble</span> <span style="color:#00a">import</span> ExtraTreesRegressor, RandomForestRegressor  <span style="color:#aaa;font-style:italic"># Optional: if used later</span>
</span></span><span style="display:flex;"><span><span style="color:#00a">from</span> <span style="color:#0aa;text-decoration:underline">sklearn.pipeline</span> <span style="color:#00a">import</span> make_pipeline, Pipeline
</span></span><span style="display:flex;"><span><span style="color:#00a">from</span> <span style="color:#0aa;text-decoration:underline">sklearn.model_selection</span> <span style="color:#00a">import</span> KFold
</span></span><span style="display:flex;"><span><span style="color:#00a">from</span> <span style="color:#0aa;text-decoration:underline">sklearn.preprocessing</span> <span style="color:#00a">import</span> StandardScaler, PolynomialFeatures, SplineTransformer
</span></span><span style="display:flex;"><span><span style="color:#00a">from</span> <span style="color:#0aa;text-decoration:underline">sklearn.metrics</span> <span style="color:#00a">import</span> r2_score
</span></span><span style="display:flex;"><span><span style="color:#00a">from</span> <span style="color:#0aa;text-decoration:underline">sklearn.decomposition</span> <span style="color:#00a">import</span> PCA
</span></span><span style="display:flex;"><span><span style="color:#00a">from</span> <span style="color:#0aa;text-decoration:underline">sklearn.manifold</span> <span style="color:#00a">import</span> TSNE
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># UMAP for dimensionality reduction</span>
</span></span><span style="display:flex;"><span><span style="color:#00a">import</span> <span style="color:#0aa;text-decoration:underline">umap</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Statistical modelling</span>
</span></span><span style="display:flex;"><span><span style="color:#00a">import</span> <span style="color:#0aa;text-decoration:underline">statsmodels.api</span> <span style="color:#00a">as</span> <span style="color:#0aa;text-decoration:underline">sm</span>
</span></span></code></pre></div><h2 id="loading-the-data">Loading the Data<a hidden class="anchor" aria-hidden="true" href="#loading-the-data">#</a></h2>
<p>We begin by loading the dataset and observing its structure. The training data consists of over one million rows and twenty features.</p>
<p><strong>Key Insights:</strong></p>
<ul>
<li>Given the dataset&rsquo;s size, it&rsquo;s important to select algorithms that scale efficiently with large volumes of data. Methods reliant on distance computations (e.g., k-nearest neighbors) or those involving kernel matrices may not be computationally feasible.</li>
<li>To optimise training time, especially during experimentation, it may be practical to use a simple train–test split instead of full cross-validation. While five-fold cross-validation offers robust performance estimates, it can be prohibitively time-consuming at this scale.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>train = pd.read_csv(<span style="color:#a50">&#39;train.csv&#39;</span>, index_col=<span style="color:#a50">&#39;id&#39;</span>)
</span></span><span style="display:flex;"><span>test = pd.read_csv(<span style="color:#a50">&#39;test.csv&#39;</span>, index_col=<span style="color:#a50">&#39;id&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>initial_features = <span style="color:#0aa">list</span>(test.columns)
</span></span><span style="display:flex;"><span>train
</span></span></code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MonsoonIntensity</th>
      <th>TopographyDrainage</th>
      <th>RiverManagement</th>
      <th>Deforestation</th>
      <th>Urbanization</th>
      <th>ClimateChange</th>
      <th>DamsQuality</th>
      <th>Siltation</th>
      <th>AgriculturalPractices</th>
      <th>Encroachments</th>
      <th>...</th>
      <th>DrainageSystems</th>
      <th>CoastalVulnerability</th>
      <th>Landslides</th>
      <th>Watersheds</th>
      <th>DeterioratingInfrastructure</th>
      <th>PopulationScore</th>
      <th>WetlandLoss</th>
      <th>InadequatePlanning</th>
      <th>PoliticalFactors</th>
      <th>FloodProbability</th>
    </tr>
    <tr>
      <th>id</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5</td>
      <td>8</td>
      <td>5</td>
      <td>8</td>
      <td>6</td>
      <td>4</td>
      <td>4</td>
      <td>3</td>
      <td>3</td>
      <td>4</td>
      <td>...</td>
      <td>5</td>
      <td>3</td>
      <td>3</td>
      <td>5</td>
      <td>4</td>
      <td>7</td>
      <td>5</td>
      <td>7</td>
      <td>3</td>
      <td>0.445</td>
    </tr>
    <tr>
      <th>1</th>
      <td>6</td>
      <td>7</td>
      <td>4</td>
      <td>4</td>
      <td>8</td>
      <td>8</td>
      <td>3</td>
      <td>5</td>
      <td>4</td>
      <td>6</td>
      <td>...</td>
      <td>7</td>
      <td>2</td>
      <td>0</td>
      <td>3</td>
      <td>5</td>
      <td>3</td>
      <td>3</td>
      <td>4</td>
      <td>3</td>
      <td>0.450</td>
    </tr>
    <tr>
      <th>2</th>
      <td>6</td>
      <td>5</td>
      <td>6</td>
      <td>7</td>
      <td>3</td>
      <td>7</td>
      <td>1</td>
      <td>5</td>
      <td>4</td>
      <td>5</td>
      <td>...</td>
      <td>7</td>
      <td>3</td>
      <td>7</td>
      <td>5</td>
      <td>6</td>
      <td>8</td>
      <td>2</td>
      <td>3</td>
      <td>3</td>
      <td>0.530</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>4</td>
      <td>6</td>
      <td>5</td>
      <td>4</td>
      <td>8</td>
      <td>4</td>
      <td>7</td>
      <td>6</td>
      <td>8</td>
      <td>...</td>
      <td>2</td>
      <td>4</td>
      <td>7</td>
      <td>4</td>
      <td>4</td>
      <td>6</td>
      <td>5</td>
      <td>7</td>
      <td>5</td>
      <td>0.535</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>3</td>
      <td>2</td>
      <td>6</td>
      <td>4</td>
      <td>4</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>...</td>
      <td>2</td>
      <td>2</td>
      <td>6</td>
      <td>6</td>
      <td>4</td>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>5</td>
      <td>0.415</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1117952</th>
      <td>3</td>
      <td>3</td>
      <td>4</td>
      <td>10</td>
      <td>4</td>
      <td>5</td>
      <td>5</td>
      <td>7</td>
      <td>10</td>
      <td>4</td>
      <td>...</td>
      <td>7</td>
      <td>8</td>
      <td>7</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>4</td>
      <td>6</td>
      <td>4</td>
      <td>0.495</td>
    </tr>
    <tr>
      <th>1117953</th>
      <td>2</td>
      <td>2</td>
      <td>4</td>
      <td>3</td>
      <td>9</td>
      <td>5</td>
      <td>8</td>
      <td>1</td>
      <td>3</td>
      <td>5</td>
      <td>...</td>
      <td>9</td>
      <td>4</td>
      <td>4</td>
      <td>3</td>
      <td>7</td>
      <td>4</td>
      <td>9</td>
      <td>4</td>
      <td>5</td>
      <td>0.480</td>
    </tr>
    <tr>
      <th>1117954</th>
      <td>7</td>
      <td>3</td>
      <td>9</td>
      <td>4</td>
      <td>6</td>
      <td>5</td>
      <td>9</td>
      <td>1</td>
      <td>3</td>
      <td>4</td>
      <td>...</td>
      <td>5</td>
      <td>5</td>
      <td>5</td>
      <td>5</td>
      <td>6</td>
      <td>5</td>
      <td>5</td>
      <td>2</td>
      <td>4</td>
      <td>0.485</td>
    </tr>
    <tr>
      <th>1117955</th>
      <td>7</td>
      <td>3</td>
      <td>3</td>
      <td>7</td>
      <td>5</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>6</td>
      <td>4</td>
      <td>...</td>
      <td>6</td>
      <td>8</td>
      <td>5</td>
      <td>3</td>
      <td>4</td>
      <td>6</td>
      <td>7</td>
      <td>6</td>
      <td>4</td>
      <td>0.495</td>
    </tr>
    <tr>
      <th>1117956</th>
      <td>4</td>
      <td>5</td>
      <td>6</td>
      <td>9</td>
      <td>5</td>
      <td>5</td>
      <td>2</td>
      <td>8</td>
      <td>4</td>
      <td>5</td>
      <td>...</td>
      <td>4</td>
      <td>8</td>
      <td>6</td>
      <td>5</td>
      <td>5</td>
      <td>6</td>
      <td>7</td>
      <td>7</td>
      <td>8</td>
      <td>0.560</td>
    </tr>
  </tbody>
</table>
<p>1117957 rows × 21 columns</p>
</div>
<h2 id="target-variable-floodprobability-distribution">Target Variable: <code>FloodProbability</code> Distribution<a hidden class="anchor" aria-hidden="true" href="#target-variable-floodprobability-distribution">#</a></h2>
<p>The target variable, <code>FloodProbability</code>, ranges from 0.285 to 0.725. All values are discrete and occur in increments of 0.005, suggesting that the target has been finely quantised, likely as a result of post-processing or model calibration.</p>
<p>To visualise the distribution, we plot a histogram using a bin width of 0.005, ensuring that each unique target value is represented in its own bin. The resulting distribution is symmetric and closely resembles a normal distribution centered around 0.5, indicating a well-balanced target variable.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>bin_edges = np.arange(<span style="color:#099">0.2825</span>, <span style="color:#099">0.7300</span>, <span style="color:#099">0.005</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt.figure(figsize=(<span style="color:#099">8</span>, <span style="color:#099">3</span>))
</span></span><span style="display:flex;"><span>plt.hist(train[<span style="color:#a50">&#39;FloodProbability&#39;</span>], bins=bin_edges, density=<span style="color:#00a">True</span>, edgecolor=<span style="color:#a50">&#39;black&#39;</span>, linewidth=<span style="color:#099">0.05</span>)
</span></span><span style="display:flex;"><span>plt.title(<span style="color:#a50">&#39;Distribution of FloodProbability&#39;</span>)
</span></span><span style="display:flex;"><span>plt.xlabel(<span style="color:#a50">&#39;FloodProbability&#39;</span>)
</span></span><span style="display:flex;"><span>plt.ylabel(<span style="color:#a50">&#39;Probability Mass&#39;</span>)
</span></span><span style="display:flex;"><span>plt.grid(axis=<span style="color:#a50">&#39;y&#39;</span>, linestyle=<span style="color:#a50">&#39;--&#39;</span>, alpha=<span style="color:#099">0.5</span>)
</span></span><span style="display:flex;"><span>plt.tight_layout()
</span></span><span style="display:flex;"><span>plt.show()
</span></span></code></pre></div><p><img loading="lazy" src="flood_prediction_files/flood_prediction_5_0.png" alt="png"  />
</p>
<h2 id="feature-distributions">Feature Distributions<a hidden class="anchor" aria-hidden="true" href="#feature-distributions">#</a></h2>
<p>All features in the dataset are integer-valued and exhibit right-skewed distributions, with the majority of values concentrated near the lower end of the range. Visual comparison of the training and test sets reveals that their feature distributions are virtually identical. Overlapping histograms show no meaningful divergence - if discrepancies existed, the bars representing each set (plotted in different colors) would be clearly distinguishable.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00a">import</span> <span style="color:#0aa;text-decoration:underline">matplotlib.pyplot</span> <span style="color:#00a">as</span> <span style="color:#0aa;text-decoration:underline">plt</span>
</span></span><span style="display:flex;"><span><span style="color:#00a">from</span> <span style="color:#0aa;text-decoration:underline">matplotlib.ticker</span> <span style="color:#00a">import</span> MaxNLocator
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Plot feature distributions for train vs test</span>
</span></span><span style="display:flex;"><span>fig, axs = plt.subplots(<span style="color:#099">5</span>, <span style="color:#099">4</span>, figsize=(<span style="color:#099">14</span>, <span style="color:#099">12</span>))  <span style="color:#aaa;font-style:italic"># Slightly wider for better spacing</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#00a">for</span> col, ax <span style="color:#00a">in</span> <span style="color:#0aa">zip</span>(initial_features, axs.ravel()):
</span></span><span style="display:flex;"><span>    <span style="color:#aaa;font-style:italic"># Relative frequency in training set</span>
</span></span><span style="display:flex;"><span>    train_dist = train[col].value_counts(normalize=<span style="color:#00a">True</span>).sort_index()
</span></span><span style="display:flex;"><span>    ax.bar(train_dist.index, train_dist.values, label=<span style="color:#a50">&#39;Train&#39;</span>, alpha=<span style="color:#099">1.0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#aaa;font-style:italic"># Relative frequency in test set</span>
</span></span><span style="display:flex;"><span>    test_dist = test[col].value_counts(normalize=<span style="color:#00a">True</span>).sort_index()
</span></span><span style="display:flex;"><span>    ax.bar(test_dist.index, test_dist.values, label=<span style="color:#a50">&#39;Test&#39;</span>, alpha=<span style="color:#099">0.6</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    ax.set_title(col)
</span></span><span style="display:flex;"><span>    ax.xaxis.set_major_locator(MaxNLocator(integer=<span style="color:#00a">True</span>))
</span></span><span style="display:flex;"><span>    ax.set_ylabel(<span style="color:#a50">&#39;Proportion&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Add legend only once, outside the grid</span>
</span></span><span style="display:flex;"><span>handles, labels = ax.get_legend_handles_labels()
</span></span><span style="display:flex;"><span>fig.legend(handles, labels, loc=<span style="color:#a50">&#39;upper right&#39;</span>, fontsize=<span style="color:#a50">&#39;medium&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt.tight_layout()
</span></span><span style="display:flex;"><span>plt.show()
</span></span></code></pre></div><p><img loading="lazy" src="flood_prediction_files/flood_prediction_7_0.png" alt="png"  />
</p>
<h2 id="correlation">Correlation<a hidden class="anchor" aria-hidden="true" href="#correlation">#</a></h2>
<p>The correlation matrix reveals several interesting patterns:</p>
<ul>
<li>There is virtually no linear correlation among the features - the pairwise correlation coefficients are all extremely close to zero.</li>
<li>However, each feature exhibits a noticeable correlation with the target variable <code>FloodProbability</code>, suggesting that while features may be mutually uncorrelated, they individually contribute predictive information.</li>
</ul>
<p>A subtle and intriguing detail emerges from the correlation heatmap: none of the feature-feature correlations are exactly <code>0.0</code>; instead, all are <code>-0.0</code>. While this might appear negligible, the consistent sign indicates that every pair of features is negatively correlated, albeit at an imperceptibly small scale. This observation hints at a form of weak statistical dependence across the feature set, despite the absence of meaningful linear correlation.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Prepare the list of features including the target</span>
</span></span><span style="display:flex;"><span>corr_features = initial_features + [<span style="color:#a50">&#39;FloodProbability&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Compute correlation matrix</span>
</span></span><span style="display:flex;"><span>correlation_matrix = np.corrcoef(train[corr_features], rowvar=<span style="color:#00a">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Plot heatmap</span>
</span></span><span style="display:flex;"><span>plt.figure(figsize=(<span style="color:#099">11</span>, <span style="color:#099">11</span>))
</span></span><span style="display:flex;"><span>sns.heatmap(
</span></span><span style="display:flex;"><span>    correlation_matrix,
</span></span><span style="display:flex;"><span>    center=<span style="color:#099">0</span>,
</span></span><span style="display:flex;"><span>    cmap=<span style="color:#a50">&#39;coolwarm&#39;</span>,
</span></span><span style="display:flex;"><span>    annot=<span style="color:#00a">True</span>,
</span></span><span style="display:flex;"><span>    fmt=<span style="color:#a50">&#39;.1f&#39;</span>,
</span></span><span style="display:flex;"><span>    xticklabels=corr_features,
</span></span><span style="display:flex;"><span>    yticklabels=corr_features,
</span></span><span style="display:flex;"><span>    square=<span style="color:#00a">True</span>,
</span></span><span style="display:flex;"><span>    linewidths=<span style="color:#099">0.5</span>,
</span></span><span style="display:flex;"><span>    cbar_kws={<span style="color:#a50">&#39;shrink&#39;</span>: <span style="color:#099">0.8</span>}
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>plt.title(<span style="color:#a50">&#39;Correlation Matrix of Features and Target&#39;</span>, fontsize=<span style="color:#099">14</span>)
</span></span><span style="display:flex;"><span>plt.tight_layout()
</span></span><span style="display:flex;"><span>plt.show()
</span></span></code></pre></div><p><img loading="lazy" src="flood_prediction_files/flood_prediction_9_0.png" alt="png"  />
</p>
<h2 id="dimensionality-reduction">Dimensionality Reduction<a hidden class="anchor" aria-hidden="true" href="#dimensionality-reduction">#</a></h2>
<h3 id="principal-component-analysis-pca">Principal Component Analysis (PCA)<a hidden class="anchor" aria-hidden="true" href="#principal-component-analysis-pca">#</a></h3>
<p>To assess the intrinsic dimensionality of the dataset, we apply Principal Component Analysis (PCA). The cumulative explained variance curve shows a gradual increase across components, indicating that the variance is distributed relatively evenly across all dimensions. This suggests that the dataset is effectively full-rank, and no small subset of principal components captures a dominant share of the variance.</p>
<p>In other words, linear dimensionality reduction would not be beneficial, as each feature contributes unique information that cannot be easily compressed without significant loss of signal.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Fit PCA on the feature set</span>
</span></span><span style="display:flex;"><span>pca = PCA()
</span></span><span style="display:flex;"><span>pca.fit(train[initial_features])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Plot cumulative explained variance</span>
</span></span><span style="display:flex;"><span>plt.figure(figsize=(<span style="color:#099">4</span>, <span style="color:#099">3</span>))
</span></span><span style="display:flex;"><span>plt.plot(np.arange(<span style="color:#099">1</span>, <span style="color:#0aa">len</span>(initial_features) + <span style="color:#099">1</span>), pca.explained_variance_ratio_.cumsum(), marker=<span style="color:#a50">&#39;o&#39;</span>)
</span></span><span style="display:flex;"><span>plt.title(<span style="color:#a50">&#39;Principal Component Analysis&#39;</span>, fontsize=<span style="color:#099">12</span>)
</span></span><span style="display:flex;"><span>plt.xlabel(<span style="color:#a50">&#39;Principal Component&#39;</span>, fontsize=<span style="color:#099">10</span>)
</span></span><span style="display:flex;"><span>plt.ylabel(<span style="color:#a50">&#39;Cumulative Explained Variance&#39;</span>, fontsize=<span style="color:#099">10</span>)
</span></span><span style="display:flex;"><span>plt.xticks(fontsize=<span style="color:#099">9</span>)
</span></span><span style="display:flex;"><span>plt.yticks([<span style="color:#099">0</span>, <span style="color:#099">0.5</span>, <span style="color:#099">1.0</span>], fontsize=<span style="color:#099">9</span>)
</span></span><span style="display:flex;"><span>plt.gca().xaxis.set_major_locator(MaxNLocator(integer=<span style="color:#00a">True</span>))
</span></span><span style="display:flex;"><span>plt.grid(<span style="color:#00a">True</span>, linestyle=<span style="color:#a50">&#39;--&#39;</span>, alpha=<span style="color:#099">0.5</span>)
</span></span><span style="display:flex;"><span>plt.tight_layout()
</span></span><span style="display:flex;"><span>plt.show()
</span></span></code></pre></div><p><img loading="lazy" src="flood_prediction_files/flood_prediction_12_0.png" alt="png"  />
</p>
<h3 id="umap">UMAP<a hidden class="anchor" aria-hidden="true" href="#umap">#</a></h3>
<p>We apply Unsupervised Uniform Manifold Approximation and Projection (UMAP) to reduce the high-dimensional feature space to two dimensions. UMAP is particularly useful for visualising structure, identifying clusters, or uncovering non-linear patterns that may not be evident in the raw feature space.</p>
<p>However, in this case, the 2D projection does not reveal any clear structure or separation with respect to the target variable, <code>FloodProbability</code>. The data appears uniformly scattered, suggesting that UMAP does not expose any obvious low-dimensional manifold in this dataset.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Function to plot the embedding</span>
</span></span><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">plot_embedding</span>(embedding, target, title):
</span></span><span style="display:flex;"><span>    plt.figure(figsize=(<span style="color:#099">6</span>, <span style="color:#099">5</span>))
</span></span><span style="display:flex;"><span>    plt.scatter(
</span></span><span style="display:flex;"><span>        embedding[:, <span style="color:#099">0</span>],
</span></span><span style="display:flex;"><span>        embedding[:, <span style="color:#099">1</span>],
</span></span><span style="display:flex;"><span>        s=<span style="color:#099">2</span>,
</span></span><span style="display:flex;"><span>        c=target,
</span></span><span style="display:flex;"><span>        cmap=<span style="color:#a50">&#39;coolwarm&#39;</span>,
</span></span><span style="display:flex;"><span>        alpha=<span style="color:#099">0.6</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    plt.gca().set_aspect(<span style="color:#a50">&#39;equal&#39;</span>, <span style="color:#a50">&#39;datalim&#39;</span>)
</span></span><span style="display:flex;"><span>    plt.title(title, fontsize=<span style="color:#099">14</span>)
</span></span><span style="display:flex;"><span>    plt.xlabel(<span style="color:#a50">&#39;UMAP Dimension 1&#39;</span>)
</span></span><span style="display:flex;"><span>    plt.ylabel(<span style="color:#a50">&#39;UMAP Dimension 2&#39;</span>)
</span></span><span style="display:flex;"><span>    plt.colorbar(label=<span style="color:#a50">&#39;FloodProbability&#39;</span>)
</span></span><span style="display:flex;"><span>    plt.tight_layout()
</span></span><span style="display:flex;"><span>    plt.show()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Sample a subset for faster projection</span>
</span></span><span style="display:flex;"><span>train_sample = train.sample(<span style="color:#099">10_000</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Fit UMAP</span>
</span></span><span style="display:flex;"><span>reducer = umap.UMAP()
</span></span><span style="display:flex;"><span>embedding = reducer.fit_transform(train_sample[initial_features])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Plot the result</span>
</span></span><span style="display:flex;"><span>plot_embedding(
</span></span><span style="display:flex;"><span>    embedding,
</span></span><span style="display:flex;"><span>    train_sample[<span style="color:#a50">&#39;FloodProbability&#39;</span>],
</span></span><span style="display:flex;"><span>    <span style="color:#a50">&#39;Unsupervised UMAP Projection of the Training Dataset&#39;</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p><img loading="lazy" src="flood_prediction_files/flood_prediction_14_1.png" alt="png"  />
</p>
<h3 id="supervised-umap-projection">Supervised UMAP Projection<a hidden class="anchor" aria-hidden="true" href="#supervised-umap-projection">#</a></h3>
<p>We also experiment with a supervised version of UMAP, which incorporates the target variable (<code>FloodProbability</code>) during the embedding process. This often enhances the separation of data based on the target and can reveal useful structure when relationships exist.</p>
<p>In this case, the resulting 2D projection does appear more organised visually. However, the structure largely reflects the quantised nature of the target, which consists of 83 discrete values. The apparent clustering is therefore an artifact of the target discretization rather than any meaningful data-driven separation. As a result, the supervised UMAP does not offer any additional insight beyond what was already known.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Sample a smaller subset for faster computation</span>
</span></span><span style="display:flex;"><span>train_sample = train.sample(<span style="color:#099">10000</span>, random_state=<span style="color:#099">42</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Fit Supervised UMAP using regression target</span>
</span></span><span style="display:flex;"><span>reducer = umap.UMAP(
</span></span><span style="display:flex;"><span>    n_neighbors=<span style="color:#099">50</span>,         <span style="color:#aaa;font-style:italic"># smaller neighborhood for faster computation</span>
</span></span><span style="display:flex;"><span>    min_dist=<span style="color:#099">0.5</span>,           <span style="color:#aaa;font-style:italic"># moderate compression</span>
</span></span><span style="display:flex;"><span>    target_metric=<span style="color:#a50">&#39;manhattan&#39;</span>,
</span></span><span style="display:flex;"><span>    target_weight=<span style="color:#099">0.6</span>,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Perform the supervised projection</span>
</span></span><span style="display:flex;"><span>embedding = reducer.fit_transform(
</span></span><span style="display:flex;"><span>    train_sample[initial_features],
</span></span><span style="display:flex;"><span>    y=train_sample[<span style="color:#a50">&#39;FloodProbability&#39;</span>]
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Plotting function</span>
</span></span><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">plot_embedding</span>(embedding, target, title):
</span></span><span style="display:flex;"><span>    plt.figure(figsize=(<span style="color:#099">6</span>, <span style="color:#099">5</span>))
</span></span><span style="display:flex;"><span>    scatter = plt.scatter(
</span></span><span style="display:flex;"><span>        embedding[:, <span style="color:#099">0</span>],
</span></span><span style="display:flex;"><span>        embedding[:, <span style="color:#099">1</span>],
</span></span><span style="display:flex;"><span>        s=<span style="color:#099">2</span>,
</span></span><span style="display:flex;"><span>        c=target,
</span></span><span style="display:flex;"><span>        cmap=<span style="color:#a50">&#39;coolwarm&#39;</span>,
</span></span><span style="display:flex;"><span>        alpha=<span style="color:#099">0.7</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    plt.gca().set_aspect(<span style="color:#a50">&#39;equal&#39;</span>, <span style="color:#a50">&#39;datalim&#39;</span>)
</span></span><span style="display:flex;"><span>    plt.title(title, fontsize=<span style="color:#099">14</span>)
</span></span><span style="display:flex;"><span>    plt.xlabel(<span style="color:#a50">&#39;UMAP Dimension 1&#39;</span>)
</span></span><span style="display:flex;"><span>    plt.ylabel(<span style="color:#a50">&#39;UMAP Dimension 2&#39;</span>)
</span></span><span style="display:flex;"><span>    cbar = plt.colorbar(scatter)
</span></span><span style="display:flex;"><span>    cbar.set_label(<span style="color:#a50">&#39;FloodProbability&#39;</span>)
</span></span><span style="display:flex;"><span>    plt.tight_layout()
</span></span><span style="display:flex;"><span>    plt.show()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Visualise the result</span>
</span></span><span style="display:flex;"><span>plot_embedding(
</span></span><span style="display:flex;"><span>    embedding,
</span></span><span style="display:flex;"><span>    train_sample[<span style="color:#a50">&#39;FloodProbability&#39;</span>],
</span></span><span style="display:flex;"><span>    <span style="color:#a50">&#39;Supervised UMAP Projection of the Training Dataset&#39;</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p><img loading="lazy" src="flood_prediction_files/flood_prediction_16_0.png" alt="png"  />
</p>
<h3 id="t-distributed-stochastic-neighbor-embedding-t-sne">t-distributed Stochastic Neighbor Embedding (t-SNE)<a hidden class="anchor" aria-hidden="true" href="#t-distributed-stochastic-neighbor-embedding-t-sne">#</a></h3>
<p>We also apply t-distributed Stochastic Neighbor Embedding (t-SNE) to project the high-dimensional feature space into two dimensions. t-SNE is a popular non-linear technique that is particularly effective at capturing local structure and revealing clusters.</p>
<p>In this case, however, the t-SNE projection does not uncover any meaningful patterns or structure in relation to the target variable, <code>FloodProbability</code>. The visualisation resembles random noise, indicating that t-SNE, like UMAP, fails to reveal a low-dimensional manifold in this dataset. Thus, t-SNE offers no additional insight for exploratory purposes.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Sample a subset for computational efficiency</span>
</span></span><span style="display:flex;"><span>train_sample = train.sample(<span style="color:#099">20_000</span>, random_state=<span style="color:#099">42</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Fit and transform with t-SNE</span>
</span></span><span style="display:flex;"><span>reducer = TSNE(random_state=<span style="color:#099">42</span>, perplexity=<span style="color:#099">30</span>, max_iter=<span style="color:#099">1000</span>, learning_rate=<span style="color:#a50">&#39;auto&#39;</span>)
</span></span><span style="display:flex;"><span>embedding = reducer.fit_transform(train_sample[initial_features])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Plot the result</span>
</span></span><span style="display:flex;"><span>plot_embedding(
</span></span><span style="display:flex;"><span>    embedding,
</span></span><span style="display:flex;"><span>    train_sample[<span style="color:#a50">&#39;FloodProbability&#39;</span>],
</span></span><span style="display:flex;"><span>    <span style="color:#a50">&#39;(Unsupervised) t-SNE Projection of the Training Dataset&#39;</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p><img loading="lazy" src="flood_prediction_files/flood_prediction_18_1.png" alt="png"  />
</p>
<h2 id="cross-validation-strategy">Cross-Validation Strategy<a hidden class="anchor" aria-hidden="true" href="#cross-validation-strategy">#</a></h2>
<p>To ensure consistency and reproducibility across all model evaluations, we define a unified cross-validation function that standardises how models are trained and validated.</p>
<p>For efficiency during early experimentation, we evaluate performance using only the first fold of the five-fold cross-validation split. This approach significantly reduces computation time while providing a reasonable estimate of model performance. If higher precision is required later in the modeling pipeline, we can easily enable all five folds by adjusting a single flag.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00a">from</span> <span style="color:#0aa;text-decoration:underline">collections</span> <span style="color:#00a">import</span> defaultdict
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Configuration</span>
</span></span><span style="display:flex;"><span>kf = KFold(n_splits=<span style="color:#099">5</span>, shuffle=<span style="color:#00a">True</span>, random_state=<span style="color:#099">1</span>)
</span></span><span style="display:flex;"><span>SINGLE_FOLD = <span style="color:#00a">True</span>       <span style="color:#aaa;font-style:italic"># Toggle for full vs. single-fold CV</span>
</span></span><span style="display:flex;"><span>COMPUTE_TEST_PRED = <span style="color:#00a">True</span> <span style="color:#aaa;font-style:italic"># Toggle to compute test set predictions</span>
</span></span><span style="display:flex;"><span>oof = defaultdict(<span style="color:#00a">lambda</span>: np.full_like(train.FloodProbability, np.nan, dtype=<span style="color:#0aa">float</span>))
</span></span><span style="display:flex;"><span>test_pred = {}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#00a">def</span> <span style="color:#0a0">cross_validate</span>(model, label, features=initial_features, n_repeats=<span style="color:#099">1</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#a50">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#a50">    Evaluate a model using cross-validation.
</span></span></span><span style="display:flex;"><span><span style="color:#a50">    
</span></span></span><span style="display:flex;"><span><span style="color:#a50">    Parameters:
</span></span></span><span style="display:flex;"><span><span style="color:#a50">    - model: scikit-learn compatible regressor
</span></span></span><span style="display:flex;"><span><span style="color:#a50">    - label: string identifier for the model
</span></span></span><span style="display:flex;"><span><span style="color:#a50">    - features: list of feature names to use
</span></span></span><span style="display:flex;"><span><span style="color:#a50">    - n_repeats: number of training repeats with different random seeds
</span></span></span><span style="display:flex;"><span><span style="color:#a50">
</span></span></span><span style="display:flex;"><span><span style="color:#a50">    Outputs:
</span></span></span><span style="display:flex;"><span><span style="color:#a50">    - Stores out-of-fold predictions in `oof[label]`
</span></span></span><span style="display:flex;"><span><span style="color:#a50">    - Stores averaged test predictions in `test_pred[label]` if enabled
</span></span></span><span style="display:flex;"><span><span style="color:#a50">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    start_time = datetime.datetime.now()
</span></span><span style="display:flex;"><span>    scores = []
</span></span><span style="display:flex;"><span>    oof_preds = np.full_like(train.FloodProbability, np.nan, dtype=<span style="color:#0aa">float</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a">for</span> fold, (idx_tr, idx_va) <span style="color:#00a">in</span> <span style="color:#0aa">enumerate</span>(kf.split(train)):
</span></span><span style="display:flex;"><span>        X_tr = train.iloc[idx_tr][features]
</span></span><span style="display:flex;"><span>        X_va = train.iloc[idx_va][features]
</span></span><span style="display:flex;"><span>        y_tr = train.iloc[idx_tr].FloodProbability
</span></span><span style="display:flex;"><span>        y_va = train.iloc[idx_va].FloodProbability
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        y_pred = np.zeros_like(y_va, dtype=<span style="color:#0aa">float</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#00a">for</span> i <span style="color:#00a">in</span> <span style="color:#0aa">range</span>(n_repeats):
</span></span><span style="display:flex;"><span>            m = clone(model)
</span></span><span style="display:flex;"><span>            <span style="color:#00a">if</span> n_repeats &gt; <span style="color:#099">1</span>:
</span></span><span style="display:flex;"><span>                mm = m
</span></span><span style="display:flex;"><span>                <span style="color:#00a">if</span> <span style="color:#0aa">isinstance</span>(mm, Pipeline):
</span></span><span style="display:flex;"><span>                    mm = mm[-<span style="color:#099">1</span>]
</span></span><span style="display:flex;"><span>                <span style="color:#00a">if</span> <span style="color:#0aa">isinstance</span>(mm, TransformedTargetRegressor):
</span></span><span style="display:flex;"><span>                    mm = mm.regressor
</span></span><span style="display:flex;"><span>                mm.set_params(random_state=i)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            m.fit(X_tr, y_tr)
</span></span><span style="display:flex;"><span>            y_pred += m.predict(X_va)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        y_pred /= n_repeats
</span></span><span style="display:flex;"><span>        score = r2_score(y_va, y_pred)
</span></span><span style="display:flex;"><span>        <span style="color:#0aa">print</span>(<span style="color:#a50">f</span><span style="color:#a50">&#34;# Fold </span><span style="color:#a50">{</span>fold<span style="color:#a50">}</span><span style="color:#a50">: R² = </span><span style="color:#a50">{</span>score<span style="color:#a50">:</span><span style="color:#a50">.5f</span><span style="color:#a50">}</span><span style="color:#a50">&#34;</span>)
</span></span><span style="display:flex;"><span>        scores.append(score)
</span></span><span style="display:flex;"><span>        oof_preds[idx_va] = y_pred
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#00a">if</span> SINGLE_FOLD:
</span></span><span style="display:flex;"><span>            <span style="color:#00a">break</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    elapsed_time = datetime.datetime.now() - start_time
</span></span><span style="display:flex;"><span>    avg_score = np.mean(scores)
</span></span><span style="display:flex;"><span>    <span style="color:#0aa">print</span>(<span style="color:#a50">f</span><span style="color:#a50">&#34;</span><span style="color:#a50">{</span>Fore.GREEN<span style="color:#a50">}</span><span style="color:#a50"># Overall R²: </span><span style="color:#a50">{</span>avg_score<span style="color:#a50">:</span><span style="color:#a50">.5f</span><span style="color:#a50">}</span><span style="color:#a50"> | Model: </span><span style="color:#a50">{</span>label<span style="color:#a50">}</span><span style="color:#a50">&#34;</span>
</span></span><span style="display:flex;"><span>          <span style="color:#a50">f</span><span style="color:#a50">&#34;</span><span style="color:#a50">{</span><span style="color:#a50">&#39; (single fold)&#39;</span> <span style="color:#00a">if</span> SINGLE_FOLD <span style="color:#00a">else</span> <span style="color:#a50">&#39;&#39;</span><span style="color:#a50">}</span><span style="color:#a50"> &#34;</span>
</span></span><span style="display:flex;"><span>          <span style="color:#a50">f</span><span style="color:#a50">&#34;| Time: </span><span style="color:#a50">{</span><span style="color:#0aa">int</span>(np.round(elapsed_time.total_seconds() / <span style="color:#099">60</span>))<span style="color:#a50">}</span><span style="color:#a50"> min</span><span style="color:#a50">{</span>Style.RESET_ALL<span style="color:#a50">}</span><span style="color:#a50">&#34;</span>)
</span></span><span style="display:flex;"><span>    oof[label] = oof_preds
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#aaa;font-style:italic"># Optional: generate test set predictions</span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a">if</span> COMPUTE_TEST_PRED:
</span></span><span style="display:flex;"><span>        X_tr = train[features]
</span></span><span style="display:flex;"><span>        y_tr = train.FloodProbability
</span></span><span style="display:flex;"><span>        y_pred = np.zeros(<span style="color:#0aa">len</span>(test), dtype=<span style="color:#0aa">float</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#00a">for</span> i <span style="color:#00a">in</span> <span style="color:#0aa">range</span>(n_repeats):
</span></span><span style="display:flex;"><span>            m = clone(model)
</span></span><span style="display:flex;"><span>            <span style="color:#00a">if</span> n_repeats &gt; <span style="color:#099">1</span>:
</span></span><span style="display:flex;"><span>                mm = m
</span></span><span style="display:flex;"><span>                <span style="color:#00a">if</span> <span style="color:#0aa">isinstance</span>(mm, Pipeline):
</span></span><span style="display:flex;"><span>                    mm = mm[-<span style="color:#099">1</span>]
</span></span><span style="display:flex;"><span>                <span style="color:#00a">if</span> <span style="color:#0aa">isinstance</span>(mm, TransformedTargetRegressor):
</span></span><span style="display:flex;"><span>                    mm = mm.regressor
</span></span><span style="display:flex;"><span>                mm.set_params(random_state=i)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            m.fit(X_tr, y_tr)
</span></span><span style="display:flex;"><span>            y_pred += m.predict(test[features])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        y_pred /= n_repeats
</span></span><span style="display:flex;"><span>        test_pred[label] = y_pred
</span></span></code></pre></div><h2 id="linear-models">Linear Models<a hidden class="anchor" aria-hidden="true" href="#linear-models">#</a></h2>
<p>We begin our modeling process with linear regression approaches, using various transformations to capture potential non-linearities in the data.</p>
<ul>
<li>A basic linear regression with standardised features provides a strong baseline.</li>
<li>Adding polynomial features improves performance slightly, suggesting that some quadratic interactions among features are predictive.</li>
<li>Applying spline transformations offers a more flexible form of non-linearity, but the performance gain is still modest.</li>
</ul>
<p>These results indicate that while the relationship between features and the target is not purely linear, simple transformations are not sufficient to fully capture the underlying patterns.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Standard Linear Regression</span>
</span></span><span style="display:flex;"><span>model = make_pipeline(
</span></span><span style="display:flex;"><span>    StandardScaler(),
</span></span><span style="display:flex;"><span>    LinearRegression()
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>cross_validate(model, <span style="color:#a50">&#39;LinearRegression&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Polynomial Features + Ridge Regression</span>
</span></span><span style="display:flex;"><span>model = make_pipeline(
</span></span><span style="display:flex;"><span>    StandardScaler(),
</span></span><span style="display:flex;"><span>    PolynomialFeatures(degree=<span style="color:#099">2</span>, include_bias=<span style="color:#00a">False</span>),
</span></span><span style="display:flex;"><span>    Ridge()
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>cross_validate(model, <span style="color:#a50">&#39;Poly-Ridge&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Spline Transformation + Ridge Regression</span>
</span></span><span style="display:flex;"><span>model = make_pipeline(
</span></span><span style="display:flex;"><span>    StandardScaler(),
</span></span><span style="display:flex;"><span>    SplineTransformer(n_knots=<span style="color:#099">5</span>, degree=<span style="color:#099">3</span>),
</span></span><span style="display:flex;"><span>    Ridge()
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>cross_validate(model, <span style="color:#a50">&#39;Spline-Ridge&#39;</span>)
</span></span></code></pre></div><pre><code># Fold 0: R² = 0.84589
[32m# Overall R²: 0.84589 | Model: LinearRegression (single fold) | Time: 0 min[0m
# Fold 0: R² = 0.84642
[32m# Overall R²: 0.84642 | Model: Poly-Ridge (single fold) | Time: 0 min[0m
# Fold 0: R² = 0.84627
[32m# Overall R²: 0.84627 | Model: Spline-Ridge (single fold) | Time: 0 min[0m
</code></pre>
<h3 id="linear-regression-with-statistical-inference">Linear Regression with Statistical Inference<a hidden class="anchor" aria-hidden="true" href="#linear-regression-with-statistical-inference">#</a></h3>
<p>As an alternative to <code>scikit-learn</code>&rsquo;s <code>LinearRegression</code>, we use <code>statsmodels</code>, which provides detailed statistical diagnostics for linear models. This implementation allows us to inspect regression coefficients, along with their associated standard errors, t-statistics, and p-values.</p>
<p>This is particularly useful during exploratory analysis to:</p>
<ul>
<li>Assess the statistical significance of each feature,</li>
<li>Interpret the magnitude and direction of feature effects,</li>
<li>Check for potential multicollinearity or model instability.</li>
</ul>
<p>While the model&rsquo;s predictive power is not the focus here, the regression summary offers valuable insights into how individual features relate to the target variable.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00a">import</span> <span style="color:#0aa;text-decoration:underline">statsmodels.api</span> <span style="color:#00a">as</span> <span style="color:#0aa;text-decoration:underline">sm</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Add intercept term</span>
</span></span><span style="display:flex;"><span>X = sm.add_constant(train[initial_features])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Fit OLS model and display summary</span>
</span></span><span style="display:flex;"><span>model = sm.OLS(train[<span style="color:#a50">&#39;FloodProbability&#39;</span>], X, missing=<span style="color:#a50">&#39;error&#39;</span>)
</span></span><span style="display:flex;"><span>results = model.fit()
</span></span><span style="display:flex;"><span>results.summary()
</span></span></code></pre></div><table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>    <td>FloodProbability</td> <th>  R-squared:         </th>  <td>   0.845</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.845</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>3.046e+05</td>
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 02 Apr 2025</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>  
</tr>
<tr>
  <th>Time:</th>                 <td>00:32:02</td>     <th>  Log-Likelihood:    </th> <td>2.7820e+06</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>1117957</td>     <th>  AIC:               </th> <td>-5.564e+06</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>1117936</td>     <th>  BIC:               </th> <td>-5.564e+06</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>    20</td>      <th>                     </th>      <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
                 <td></td>                    <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th>                           <td>   -0.0533</td> <td>    0.000</td> <td> -234.995</td> <td> 0.000</td> <td>   -0.054</td> <td>   -0.053</td>
</tr>
<tr>
  <th>MonsoonIntensity</th>                <td>    0.0056</td> <td> 9.25e-06</td> <td>  606.734</td> <td> 0.000</td> <td>    0.006</td> <td>    0.006</td>
</tr>
<tr>
  <th>TopographyDrainage</th>              <td>    0.0056</td> <td> 9.09e-06</td> <td>  621.525</td> <td> 0.000</td> <td>    0.006</td> <td>    0.006</td>
</tr>
<tr>
  <th>RiverManagement</th>                 <td>    0.0057</td> <td> 9.18e-06</td> <td>  617.178</td> <td> 0.000</td> <td>    0.006</td> <td>    0.006</td>
</tr>
<tr>
  <th>Deforestation</th>                   <td>    0.0057</td> <td> 9.27e-06</td> <td>  612.404</td> <td> 0.000</td> <td>    0.006</td> <td>    0.006</td>
</tr>
<tr>
  <th>Urbanization</th>                    <td>    0.0057</td> <td> 9.14e-06</td> <td>  619.319</td> <td> 0.000</td> <td>    0.006</td> <td>    0.006</td>
</tr>
<tr>
  <th>ClimateChange</th>                   <td>    0.0057</td> <td> 9.25e-06</td> <td>  612.437</td> <td> 0.000</td> <td>    0.006</td> <td>    0.006</td>
</tr>
<tr>
  <th>DamsQuality</th>                     <td>    0.0057</td> <td> 9.13e-06</td> <td>  619.170</td> <td> 0.000</td> <td>    0.006</td> <td>    0.006</td>
</tr>
<tr>
  <th>Siltation</th>                       <td>    0.0056</td> <td> 9.21e-06</td> <td>  612.284</td> <td> 0.000</td> <td>    0.006</td> <td>    0.006</td>
</tr>
<tr>
  <th>AgriculturalPractices</th>           <td>    0.0056</td> <td>  9.2e-06</td> <td>  612.643</td> <td> 0.000</td> <td>    0.006</td> <td>    0.006</td>
</tr>
<tr>
  <th>Encroachments</th>                   <td>    0.0056</td> <td> 9.14e-06</td> <td>  618.374</td> <td> 0.000</td> <td>    0.006</td> <td>    0.006</td>
</tr>
<tr>
  <th>IneffectiveDisasterPreparedness</th> <td>    0.0056</td> <td> 9.16e-06</td> <td>  615.995</td> <td> 0.000</td> <td>    0.006</td> <td>    0.006</td>
</tr>
<tr>
  <th>DrainageSystems</th>                 <td>    0.0056</td> <td> 9.18e-06</td> <td>  613.641</td> <td> 0.000</td> <td>    0.006</td> <td>    0.006</td>
</tr>
<tr>
  <th>CoastalVulnerability</th>            <td>    0.0057</td> <td> 9.11e-06</td> <td>  622.228</td> <td> 0.000</td> <td>    0.006</td> <td>    0.006</td>
</tr>
<tr>
  <th>Landslides</th>                      <td>    0.0056</td> <td> 9.15e-06</td> <td>  616.245</td> <td> 0.000</td> <td>    0.006</td> <td>    0.006</td>
</tr>
<tr>
  <th>Watersheds</th>                      <td>    0.0056</td> <td> 9.14e-06</td> <td>  617.853</td> <td> 0.000</td> <td>    0.006</td> <td>    0.006</td>
</tr>
<tr>
  <th>DeterioratingInfrastructure</th>     <td>    0.0056</td> <td> 9.21e-06</td> <td>  609.647</td> <td> 0.000</td> <td>    0.006</td> <td>    0.006</td>
</tr>
<tr>
  <th>PopulationScore</th>                 <td>    0.0057</td> <td> 9.17e-06</td> <td>  618.914</td> <td> 0.000</td> <td>    0.006</td> <td>    0.006</td>
</tr>
<tr>
  <th>WetlandLoss</th>                     <td>    0.0056</td> <td>  9.2e-06</td> <td>  612.654</td> <td> 0.000</td> <td>    0.006</td> <td>    0.006</td>
</tr>
<tr>
  <th>InadequatePlanning</th>              <td>    0.0056</td> <td> 9.14e-06</td> <td>  613.363</td> <td> 0.000</td> <td>    0.006</td> <td>    0.006</td>
</tr>
<tr>
  <th>PoliticalFactors</th>                <td>    0.0056</td> <td>  9.1e-06</td> <td>  620.512</td> <td> 0.000</td> <td>    0.006</td> <td>    0.006</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>100155.250</td> <th>  Durbin-Watson:     </th>  <td>   2.000</td> 
</tr>
<tr>
  <th>Prob(Omnibus):</th>   <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>148528.907</td>
</tr>
<tr>
  <th>Skew:</th>            <td> 0.703</td>   <th>  Prob(JB):          </th>  <td>    0.00</td> 
</tr>
<tr>
  <th>Kurtosis:</th>        <td> 4.100</td>   <th>  Cond. No.          </th>  <td>    265.</td> 
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
<h3 id="additive-feature-structure">Additive Feature Structure<a hidden class="anchor" aria-hidden="true" href="#additive-feature-structure">#</a></h3>
<p>An inspection of the linear regression coefficients reveals that all features have approximately the same multiplicative coefficient, estimated to be around <strong>0.00565</strong>. This uniformity suggests that the model effectively reduces to a <strong>weighted sum of features</strong> with a common weight, plus an intercept term.</p>
<p>The model can be approximated as:</p>
<p>$$
\hat{y} = \beta \cdot \sum_{i=1}^{d} x_i + \alpha
$$</p>
<p>where:</p>
<ul>
<li>$\beta \approx 0.00565$,</li>
<li>$\alpha \approx -0.053$,</li>
<li>$x_i$ represents each feature,</li>
<li>$d$ is the number of features.</li>
</ul>
<p>This implies that the sum of all feature values is a strong linear predictor of the target. Using this simplified form, we can compute an approximate prediction for <code>FloodProbability</code>:</p>
<h3 id="insight">Insight:<a hidden class="anchor" aria-hidden="true" href="#insight">#</a></h3>
<p>This surprisingly effective approximation highlights a latent structure in the dataset: the total feature magnitude strongly correlates with flood probability, even without modeling feature-specific effects.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Compute approximation using summed features</span>
</span></span><span style="display:flex;"><span>approx_prediction = train[initial_features].sum(axis=<span style="color:#099">1</span>) * <span style="color:#099">0.00565</span> - <span style="color:#099">0.053</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Evaluate goodness of fit</span>
</span></span><span style="display:flex;"><span>score = r2_score(train[<span style="color:#a50">&#39;FloodProbability&#39;</span>], approx_prediction)
</span></span><span style="display:flex;"><span><span style="color:#0aa">print</span>(<span style="color:#a50">f</span><span style="color:#a50">&#34;R² Score (approximate model): </span><span style="color:#a50">{</span>score<span style="color:#a50">:</span><span style="color:#a50">.5f</span><span style="color:#a50">}</span><span style="color:#a50">&#34;</span>)
</span></span></code></pre></div><pre><code>R² Score (approximate model): 0.84476
</code></pre>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://shaunyap01.github.io/tags/python/">`Python</a></li>
      <li><a href="https://shaunyap01.github.io/tags/all-projects/">^^All Projects</a></li>
      <li><a href="https://shaunyap01.github.io/tags/kaggle-competition/">^Kaggle Competition</a></li>
      <li><a href="https://shaunyap01.github.io/tags/exploratory-data-analysis/">Exploratory Data Analysis</a></li>
      <li><a href="https://shaunyap01.github.io/tags/flood-prediction/">Flood Prediction</a></li>
      <li><a href="https://shaunyap01.github.io/tags/dimensionality-reduction/">Dimensionality Reduction</a></li>
      <li><a href="https://shaunyap01.github.io/tags/data-visualisation/">Data Visualisation</a></li>
      <li><a href="https://shaunyap01.github.io/tags/large-dataset/">Large Dataset</a></li>
    </ul>
  </footer>
</article>
    </main>
    

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
